{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOwYjVBXKdv0uey2QEwF3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechnicalClubRBU-CodeRush1-0/CNN-LSTM/blob/main/CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment setup and library imports for CNN-LSTM portfolio manager\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import requests\n",
        "import warnings\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Intelligent Portfolio Manager Initializing...\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "gpu_count = len(tf.config.list_physical_devices('GPU'))\n",
        "print(f\"GPU acceleration: {'Enabled' if gpu_count > 0 else 'CPU mode'} ({gpu_count} devices)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP3p2gYoG0zi",
        "outputId": "b3b78673-20f9-4b92-f67f-cf3227ee0137"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intelligent Portfolio Manager Initializing...\n",
            "TensorFlow version: 2.19.0\n",
            "GPU acceleration: Enabled (1 devices)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core data structures for predictions and market data\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    symbol: str\n",
        "    action: str\n",
        "    confidence: float\n",
        "    current_price: float\n",
        "    technical_indicators: Dict[str, float]\n",
        "    data_source: str\n",
        "    timestamp: str\n",
        "    model_version: str = \"1.0.0\"\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'symbol': self.symbol,\n",
        "            'action': self.action,\n",
        "            'confidence': round(self.confidence, 4),\n",
        "            'current_price': round(self.current_price, 2),\n",
        "            'technical_indicators': {k: round(v, 4) for k, v in self.technical_indicators.items()},\n",
        "            'data_source': self.data_source,\n",
        "            'timestamp': self.timestamp,\n",
        "            'model_version': self.model_version\n",
        "        }\n",
        "\n",
        "@dataclass\n",
        "class MarketData:\n",
        "    symbol: str\n",
        "    data: pd.DataFrame\n",
        "    source: str\n",
        "    timestamp: str\n",
        "    is_mock: bool = False\n",
        "\n",
        "print(\"Core data structures configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGJkwX9jG7B6",
        "outputId": "12d29720-5b97-47b5-968a-e8d3b73fcd73"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core data structures configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced data manager with NSE fix\n",
        "\n",
        "class RobustDataManager:\n",
        "    def __init__(self, alpha_vantage_key: Optional[str] = None, fmp_key: Optional[str] = None,\n",
        "                 twelve_data_key: Optional[str] = None):\n",
        "        self.alpha_vantage_key = alpha_vantage_key or \"demo\"\n",
        "        self.fmp_key = fmp_key or \"demo\"\n",
        "        self.twelve_data_key = twelve_data_key or \"demo\"\n",
        "        self.data_sources = ['yfinance', 'alpha_vantage', 'fmp']\n",
        "        self.cache = {}\n",
        "\n",
        "    def fetch_yfinance(self, symbol: str, period: int = 120) -> Optional[pd.DataFrame]:\n",
        "        try:\n",
        "            logger.info(f\"Fetching {symbol} from YFinance...\")\n",
        "            data = yf.download(symbol, period=f'{period}d', progress=False, threads=True)\n",
        "\n",
        "            if data.empty:\n",
        "                raise ValueError(\"YFinance returned empty dataset\")\n",
        "\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "            if 'Adj Close' in data.columns:\n",
        "                data = data.drop('Adj Close', axis=1)\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"YFinance failed for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_alpha_vantage(self, symbol: str) -> Optional[pd.DataFrame]:\n",
        "        try:\n",
        "            logger.info(f\"Fetching {symbol} from Alpha Vantage...\")\n",
        "            base_symbol = symbol.replace('.NS', '')\n",
        "            url = \"https://www.alphavantage.co/query\"\n",
        "            params = {\n",
        "                'function': 'TIME_SERIES_DAILY',\n",
        "                'symbol': f'{base_symbol}.BSE',\n",
        "                'apikey': self.alpha_vantage_key,\n",
        "                'outputsize': 'full'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=15)\n",
        "            data = response.json()\n",
        "\n",
        "            if 'Time Series (Daily)' not in data:\n",
        "                raise ValueError(f\"Alpha Vantage API error: {data.get('Note', 'Unknown error')}\")\n",
        "\n",
        "            time_series = data['Time Series (Daily)']\n",
        "            df = pd.DataFrame.from_dict(time_series, orient='index')\n",
        "            df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.astype(float).sort_index()\n",
        "\n",
        "            return df.tail(120)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Alpha Vantage failed for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_fmp(self, symbol: str) -> Optional[pd.DataFrame]:\n",
        "        try:\n",
        "            logger.info(f\"Fetching {symbol} from Financial Modeling Prep...\")\n",
        "            base_symbol = symbol.replace('.NS', '.BSE')\n",
        "            url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{base_symbol}\"\n",
        "            params = {\n",
        "                'apikey': self.fmp_key,\n",
        "                'from': (datetime.now() - timedelta(days=130)).strftime('%Y-%m-%d'),\n",
        "                'to': datetime.now().strftime('%Y-%m-%d')\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=15)\n",
        "            data = response.json()\n",
        "\n",
        "            if 'historical' not in data or not data['historical']:\n",
        "                raise ValueError(\"FMP returned no historical data\")\n",
        "\n",
        "            df = pd.DataFrame(data['historical'])\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.set_index('date').sort_index()\n",
        "\n",
        "            column_mapping = {\n",
        "                'open': 'Open', 'high': 'High', 'low': 'Low',\n",
        "                'close': 'Close', 'volume': 'Volume'\n",
        "            }\n",
        "            df = df.rename(columns=column_mapping)\n",
        "\n",
        "            return df[['Open', 'High', 'Low', 'Close', 'Volume']].tail(120)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"FMP failed for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_market_data(self, symbol: str) -> MarketData:\n",
        "        active_sources = ['yfinance', 'alpha_vantage', 'fmp']\n",
        "\n",
        "        for source in active_sources:\n",
        "            try:\n",
        "                data = None\n",
        "                if source == 'yfinance':\n",
        "                    data = self.fetch_yfinance(symbol)\n",
        "                elif source == 'alpha_vantage':\n",
        "                    data = self.fetch_alpha_vantage(symbol)\n",
        "                elif source == 'fmp':\n",
        "                    data = self.fetch_fmp(symbol)\n",
        "\n",
        "                if data is not None and not data.empty and len(data) >= 60:\n",
        "                    market_data = MarketData(\n",
        "                        symbol=symbol,\n",
        "                        data=data,\n",
        "                        source=source,\n",
        "                        timestamp=datetime.now().isoformat(),\n",
        "                        is_mock=False\n",
        "                    )\n",
        "\n",
        "                    logger.info(f\"Successfully fetched {symbol} from {source} ({len(data)} rows)\")\n",
        "                    return market_data\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error with {source} for {symbol}: {e}\")\n",
        "                continue\n",
        "\n",
        "        raise RuntimeError(f\"All data sources failed for {symbol}\")\n",
        "\n",
        "print(\"Data manager configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0GGBswAcKgZ",
        "outputId": "f7a59b6d-4ee6-4d37-8829-20f1cc7d0e1c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data manager configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed advanced feature engineering with proper Series handling\n",
        "\n",
        "class AdvancedFeatureEngineer:\n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "\n",
        "    def _ensure_series(self, result, name=\"calculation\"):\n",
        "        if isinstance(result, pd.DataFrame):\n",
        "            if len(result.columns) == 1:\n",
        "                return result.iloc[:, 0]\n",
        "            else:\n",
        "                logger.warning(f\"{name} returned DataFrame with multiple columns, using first column\")\n",
        "                return result.iloc[:, 0]\n",
        "        return result\n",
        "\n",
        "    def add_price_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        df['Returns'] = df['Close'].pct_change()\n",
        "        df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "        df['Price_Range'] = (df['High'] - df['Low']) / df['Close']\n",
        "        df['Gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)\n",
        "        df['Body_Size'] = abs(df['Close'] - df['Open']) / df['Close']\n",
        "        df['Upper_Shadow'] = (df['High'] - np.maximum(df['Open'], df['Close'])) / df['Close']\n",
        "        df['Lower_Shadow'] = (np.minimum(df['Open'], df['Close']) - df['Low']) / df['Close']\n",
        "        return df\n",
        "\n",
        "    def add_moving_averages(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        periods = [3, 5, 8, 13, 21, 34, 55]\n",
        "        for period in periods:\n",
        "            sma_col = df['Close'].rolling(window=period).mean()\n",
        "            sma_col = self._ensure_series(sma_col, f\"SMA_{period}\")\n",
        "            df[f'SMA_{period}'] = sma_col\n",
        "            ratio_calc = df['Close'] / sma_col\n",
        "            ratio_calc = self._ensure_series(ratio_calc, f\"SMA_{period}_Ratio\")\n",
        "            df[f'SMA_{period}_Ratio'] = ratio_calc\n",
        "            slope_calc = sma_col.diff(5) / sma_col\n",
        "            slope_calc = self._ensure_series(slope_calc, f\"SMA_{period}_Slope\")\n",
        "            df[f'SMA_{period}_Slope'] = slope_calc\n",
        "\n",
        "        ema_periods = [8, 13, 21, 34, 55]\n",
        "        for period in ema_periods:\n",
        "            ema_col = df['Close'].ewm(span=period).mean()\n",
        "            ema_col = self._ensure_series(ema_col, f\"EMA_{period}\")\n",
        "            df[f'EMA_{period}'] = ema_col\n",
        "            ratio_calc = df['Close'] / ema_col\n",
        "            ratio_calc = self._ensure_series(ratio_calc, f\"EMA_{period}_Ratio\")\n",
        "            df[f'EMA_{period}_Ratio'] = ratio_calc\n",
        "        return df\n",
        "\n",
        "    def add_momentum_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        delta = df['Close'].diff()\n",
        "        gain = delta.where(delta > 0, 0).rolling(window=21).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=21).mean()\n",
        "        gain = self._ensure_series(gain, \"RSI_gain\")\n",
        "        loss = self._ensure_series(loss, \"RSI_loss\")\n",
        "        rs = gain / loss\n",
        "        rs = self._ensure_series(rs, \"RS\")\n",
        "        df['RSI'] = 100 - (100 / (1 + rs))\n",
        "        df['RSI_Momentum'] = df['RSI'].diff(5)\n",
        "\n",
        "        ema_12 = self._ensure_series(df['Close'].ewm(span=12).mean(), \"EMA_12\")\n",
        "        ema_26 = self._ensure_series(df['Close'].ewm(span=26).mean(), \"EMA_26\")\n",
        "        df['EMA_12'] = ema_12\n",
        "        df['EMA_26'] = ema_26\n",
        "        df['MACD'] = ema_12 - ema_26\n",
        "        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "        df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "        df['MACD_Slope'] = df['MACD'].diff(3)\n",
        "\n",
        "        low_21 = self._ensure_series(df['Low'].rolling(21).min(), \"Low_21\")\n",
        "        high_21 = self._ensure_series(df['High'].rolling(21).max(), \"High_21\")\n",
        "        df['Stoch_K'] = 100 * ((df['Close'] - low_21) / (high_21 - low_21))\n",
        "        df['Stoch_D'] = df['Stoch_K'].rolling(5).mean()\n",
        "        df['Williams_R'] = -100 * ((high_21 - df['Close']) / (high_21 - low_21))\n",
        "        df['ROC'] = ((df['Close'] - df['Close'].shift(13)) / df['Close'].shift(13)) * 100\n",
        "        df['Momentum'] = df['Close'] - df['Close'].shift(13)\n",
        "        return df\n",
        "\n",
        "    def add_volatility_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        bb_middle = self._ensure_series(df['Close'].rolling(21).mean(), \"BB_Middle\")\n",
        "        bb_std = self._ensure_series(df['Close'].rolling(21).std(), \"BB_Std\")\n",
        "        df['BB_Middle'] = bb_middle\n",
        "        df['BB_Upper'] = bb_middle + (bb_std * 2.5)\n",
        "        df['BB_Lower'] = bb_middle - (bb_std * 2.5)\n",
        "        bb_width_calc = (df['BB_Upper'] - df['BB_Lower']) / bb_middle\n",
        "        df['BB_Width'] = self._ensure_series(bb_width_calc, \"BB_Width\")\n",
        "        bb_position_calc = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "        df['BB_Position'] = self._ensure_series(bb_position_calc, \"BB_Position\")\n",
        "        bb_squeeze_calc = bb_std / bb_middle\n",
        "        df['BB_Squeeze'] = self._ensure_series(bb_squeeze_calc, \"BB_Squeeze\")\n",
        "\n",
        "        tr1 = df['High'] - df['Low']\n",
        "        tr2 = abs(df['High'] - df['Close'].shift(1))\n",
        "        tr3 = abs(df['Low'] - df['Close'].shift(1))\n",
        "        tr_combined = pd.concat([tr1, tr2, tr3], axis=1)\n",
        "        df['TR'] = tr_combined.max(axis=1)\n",
        "        atr = self._ensure_series(df['TR'].rolling(21).mean(), \"ATR\")\n",
        "        df['ATR'] = atr\n",
        "        df['ATR_Ratio'] = self._ensure_series(atr / df['Close'], \"ATR_Ratio\")\n",
        "\n",
        "        vol_5 = self._ensure_series(df['Returns'].rolling(5).std() * np.sqrt(252), \"Vol_5\")\n",
        "        vol_13 = self._ensure_series(df['Returns'].rolling(13).std() * np.sqrt(252), \"Vol_13\")\n",
        "        vol_21 = self._ensure_series(df['Returns'].rolling(21).std() * np.sqrt(252), \"Vol_21\")\n",
        "        df['Volatility_5'] = vol_5\n",
        "        df['Volatility_13'] = vol_13\n",
        "        df['Volatility_21'] = vol_21\n",
        "        df['Volatility_Ratio'] = self._ensure_series(vol_5 / vol_21, \"Volatility_Ratio\")\n",
        "        return df\n",
        "\n",
        "    def add_volume_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        vol_sma_5 = self._ensure_series(df['Volume'].rolling(5).mean(), \"Volume_SMA_5\")\n",
        "        vol_sma_13 = self._ensure_series(df['Volume'].rolling(13).mean(), \"Volume_SMA_13\")\n",
        "        vol_sma_21 = self._ensure_series(df['Volume'].rolling(21).mean(), \"Volume_SMA_21\")\n",
        "        df['Volume_SMA_5'] = vol_sma_5\n",
        "        df['Volume_SMA_13'] = vol_sma_13\n",
        "        df['Volume_SMA_21'] = vol_sma_21\n",
        "        df['Volume_Ratio_5'] = self._ensure_series(df['Volume'] / vol_sma_5, \"Volume_Ratio_5\")\n",
        "        df['Volume_Ratio_21'] = self._ensure_series(df['Volume'] / vol_sma_21, \"Volume_Ratio_21\")\n",
        "        df['Volume_Price_Trend'] = (df['Volume'] * df['Returns']).cumsum()\n",
        "        df['Price_Volume'] = df['Close'] * df['Volume']\n",
        "        volume_price_sum = (df['Close'] * df['Volume']).rolling(13).sum()\n",
        "        volume_sum = df['Volume'].rolling(13).sum()\n",
        "        vwp_calc = volume_price_sum / volume_sum\n",
        "        df['Volume_Weighted_Price'] = self._ensure_series(vwp_calc, \"Volume_Weighted_Price\")\n",
        "        return df\n",
        "\n",
        "    def create_sequences(self, df: pd.DataFrame, sequence_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        feature_cols = [\n",
        "            'Returns', 'Log_Returns', 'Price_Range', 'Body_Size', 'Upper_Shadow', 'Lower_Shadow',\n",
        "            'SMA_3_Ratio', 'SMA_5_Ratio', 'SMA_8_Ratio', 'SMA_13_Ratio', 'SMA_21_Ratio',\n",
        "            'EMA_8_Ratio', 'EMA_13_Ratio', 'EMA_21_Ratio', 'EMA_34_Ratio',\n",
        "            'RSI', 'RSI_Momentum', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'MACD_Slope',\n",
        "            'Stoch_K', 'Stoch_D', 'Williams_R', 'ROC', 'Momentum',\n",
        "            'BB_Position', 'BB_Width', 'BB_Squeeze', 'ATR_Ratio',\n",
        "            'Volatility_5', 'Volatility_13', 'Volatility_21', 'Volatility_Ratio',\n",
        "            'Volume_Ratio_5', 'Volume_Ratio_21', 'Volume_Price_Trend'\n",
        "        ]\n",
        "\n",
        "        available_cols = [col for col in feature_cols if col in df.columns]\n",
        "        self.feature_names = available_cols\n",
        "        features = df[available_cols].fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "        X, y = [], []\n",
        "        for i in range(sequence_length, len(features) - 1):\n",
        "            X.append(features.iloc[i-sequence_length:i].values)\n",
        "            current_price = df['Close'].iloc[i]\n",
        "            future_price = df['Close'].iloc[i+1]\n",
        "            if pd.isna(current_price) or pd.isna(future_price) or current_price == 0:\n",
        "                continue\n",
        "            future_return = (future_price - current_price) / current_price\n",
        "            if future_return > 0.02:\n",
        "                label = 2\n",
        "            elif future_return < -0.02:\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "            y.append(label)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y, dtype=np.int32)\n",
        "        valid_indices = ~(np.isnan(X).any(axis=(1,2)) | np.isnan(y))\n",
        "        X = X[valid_indices]\n",
        "        y = y[valid_indices]\n",
        "        logger.info(f\"Created {len(X)} valid sequences\")\n",
        "        logger.info(f\"Label distribution: SELL:{np.sum(y==0)}, HOLD:{np.sum(y==1)}, BUY:{np.sum(y==2)}\")\n",
        "        return X, y\n",
        "\n",
        "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            df = self.add_price_features(df)\n",
        "            df = self.add_moving_averages(df)\n",
        "            df = self.add_momentum_indicators(df)\n",
        "            df = self.add_volatility_indicators(df)\n",
        "            df = self.add_volume_indicators(df)\n",
        "            df = df.fillna(method='bfill').fillna(method='ffill')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Feature engineering failed: {e}\")\n",
        "            raise\n",
        "\n",
        "print(\"Feature engineering configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcGckyyBcxrI",
        "outputId": "94c04473-26d8-447b-a1fc-1ec3cd654769"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineering configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLSTMModel:\n",
        "    def __init__(self, sequence_length: int = 60, n_features: int = 35):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.n_features = n_features\n",
        "        self.model = None\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.is_trained = False\n",
        "        self.history = None\n",
        "\n",
        "    def build_model(self) -> tf.keras.Model:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu',\n",
        "                                  input_shape=(self.sequence_length, self.n_features)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.3),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LSTM(25, dropout=0.3),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(3, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def validate_data(self, X: np.ndarray, y: np.ndarray):\n",
        "        logger.info(\"Validating training data...\")\n",
        "        if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
        "            logger.error(\"Found NaN or infinite values in X\")\n",
        "            raise ValueError(\"Training data X contains NaN or infinite values\")\n",
        "        unique_labels = np.unique(y)\n",
        "        valid_labels = {0, 1, 2}\n",
        "        logger.info(f\"Unique labels found: {unique_labels}\")\n",
        "        logger.info(f\"Label distribution: {np.bincount(y)}\")\n",
        "        if not set(unique_labels).issubset(valid_labels):\n",
        "            invalid_labels = set(unique_labels) - valid_labels\n",
        "            logger.error(f\"Invalid labels found: {invalid_labels}\")\n",
        "            raise ValueError(f\"Labels must be 0, 1, or 2. Found invalid labels: {invalid_labels}\")\n",
        "        if np.any(np.isnan(y)):\n",
        "            logger.error(\"Found NaN values in labels y\")\n",
        "            raise ValueError(\"Labels y contain NaN values\")\n",
        "        logger.info(\"Data validation passed!\")\n",
        "\n",
        "    def train(self, X: np.ndarray, y: np.ndarray, validation_split: float = 0.25):\n",
        "        logger.info(f\"Training CNN-LSTM model on {len(X)} sequences...\")\n",
        "        logger.info(f\"Feature dimensions: {X.shape}\")\n",
        "        self.validate_data(X, y)\n",
        "        X_scaled = self.scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        y = y.astype(np.int32)\n",
        "        self.model = self.build_model()\n",
        "        logger.info(f\"Model parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                patience=50,\n",
        "                restore_best_weights=True,\n",
        "                monitor='val_loss',\n",
        "                mode='min'\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                patience=20,\n",
        "                factor=0.5,\n",
        "                min_lr=1e-6,\n",
        "                monitor='val_loss'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_scaled, y,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        self.is_trained = True\n",
        "        best_val_acc = max(self.history.history['val_accuracy'])\n",
        "        final_loss = self.history.history['loss'][-1]\n",
        "        logger.info(f\"Training completed!\")\n",
        "        logger.info(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "        logger.info(f\"Final loss: {final_loss:.4f}\")\n",
        "        return self.history\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "        X_scaled = self.scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        predictions = self.model.predict(X_scaled, verbose=0)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "        confidence_scores = np.max(predictions, axis=1)\n",
        "        return predicted_classes, confidence_scores\n",
        "\n",
        "print(\"CNN-LSTM model configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG8u1de7cz_8",
        "outputId": "b0148544-ed70-444b-b2c5-458ade51b1ab"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN-LSTM model configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main portfolio management system integrating all fixed components for NSE stock analysis\n",
        "\n",
        "class CNNLSTMModel:\n",
        "    def __init__(self, sequence_length: int = 60, n_features: int = 35):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.n_features = n_features\n",
        "        self.model = None\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.is_trained = False\n",
        "        self.history = None\n",
        "\n",
        "    def build_model(self) -> tf.keras.Model:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu',\n",
        "                                  input_shape=(self.sequence_length, self.n_features)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.3),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LSTM(25, dropout=0.3),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(3, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def validate_data(self, X: np.ndarray, y: np.ndarray):\n",
        "        logger.info(\"Validating training data...\")\n",
        "        if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
        "            logger.error(\"Found NaN or infinite values in X\")\n",
        "            raise ValueError(\"Training data X contains NaN or infinite values\")\n",
        "        unique_labels = np.unique(y)\n",
        "        valid_labels = {0, 1, 2}\n",
        "        logger.info(f\"Unique labels found: {unique_labels}\")\n",
        "        logger.info(f\"Label distribution: {np.bincount(y)}\")\n",
        "        if not set(unique_labels).issubset(valid_labels):\n",
        "            invalid_labels = set(unique_labels) - valid_labels\n",
        "            logger.error(f\"Invalid labels found: {invalid_labels}\")\n",
        "            raise ValueError(f\"Labels must be 0, 1, or 2. Found invalid labels: {invalid_labels}\")\n",
        "        if np.any(np.isnan(y)):\n",
        "            logger.error(\"Found NaN values in labels y\")\n",
        "            raise ValueError(\"Labels y contain NaN values\")\n",
        "        logger.info(\"Data validation passed!\")\n",
        "\n",
        "    def train(self, X: np.ndarray, y: np.ndarray, validation_split: float = 0.25):\n",
        "        logger.info(f\"Training CNN-LSTM model on {len(X)} sequences...\")\n",
        "        logger.info(f\"Feature dimensions: {X.shape}\")\n",
        "        self.validate_data(X, y)\n",
        "        X_scaled = self.scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        y = y.astype(np.int32)\n",
        "        self.model = self.build_model()\n",
        "        logger.info(f\"Model parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                patience=50,\n",
        "                restore_best_weights=True,\n",
        "                monitor='val_loss',\n",
        "                mode='min'\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                patience=20,\n",
        "                factor=0.5,\n",
        "                min_lr=1e-6,\n",
        "                monitor='val_loss'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_scaled, y,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        self.is_trained = True\n",
        "        best_val_acc = max(self.history.history['val_accuracy'])\n",
        "        final_loss = self.history.history['loss'][-1]\n",
        "        logger.info(f\"Training completed!\")\n",
        "        logger.info(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "        logger.info(f\"Final loss: {final_loss:.4f}\")\n",
        "        return self.history\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "        X_scaled = self.scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        predictions = self.model.predict(X_scaled, verbose=0)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "        confidence_scores = np.max(predictions, axis=1)\n",
        "        return predicted_classes, confidence_scores\n",
        "\n",
        "print(\"CNN-LSTM model configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHI1SrxdUEv",
        "outputId": "3e7324e3-3441-45b9-f66b-cc507fb3afb2"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN-LSTM model configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model persistence and utility functions for deployment\n",
        "\n",
        "def save_nse_portfolio_model(portfolio_manager: RobustPortfolioManager,\n",
        "                           model_path: str = \"nse_portfolio_model_v2\") -> str:\n",
        "    if not portfolio_manager.model.is_trained:\n",
        "        raise ValueError(\"Cannot save untrained model!\")\n",
        "\n",
        "    logger.info(f\"Saving NSE portfolio model to {model_path}/...\")\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    os.makedirs(f\"{model_path}/artifacts\", exist_ok=True)\n",
        "\n",
        "    tf.saved_model.save(portfolio_manager.model.model, f\"{model_path}/saved_model\")\n",
        "    logger.info(\"TensorFlow model saved in SavedModel format\")\n",
        "\n",
        "    joblib.dump(portfolio_manager.model.scaler, f\"{model_path}/artifacts/scaler.pkl\")\n",
        "    joblib.dump(portfolio_manager.feature_engineer.feature_names,\n",
        "                f\"{model_path}/artifacts/feature_names.pkl\")\n",
        "    logger.info(\"Preprocessing components saved\")\n",
        "\n",
        "    metadata = {\n",
        "        'model_info': portfolio_manager.model_metadata,\n",
        "        'training_history': portfolio_manager.training_history,\n",
        "        'feature_engineering': {\n",
        "            'sequence_length': portfolio_manager.model.sequence_length,\n",
        "            'n_features': portfolio_manager.model.n_features,\n",
        "            'feature_names': portfolio_manager.feature_engineer.feature_names\n",
        "        },\n",
        "        'model_architecture': {\n",
        "            'type': 'CNN-LSTM',\n",
        "            'action_map': portfolio_manager.action_map,\n",
        "            'thresholds': {'buy': 0.02, 'sell': -0.02}\n",
        "        },\n",
        "        'deployment_info': {\n",
        "            'saved_at': datetime.now().isoformat(),\n",
        "            'tensorflow_version': tf.__version__,\n",
        "            'fixes_applied': ['multi_level_columns', 'dataframe_series_conversion', 'savedmodel_format']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(f\"{model_path}/model_metadata.json\", 'w') as f:\n",
        "        json.dump(metadata, f, indent=2, default=str)\n",
        "\n",
        "    logger.info(f\"Model package saved successfully to {model_path}/\")\n",
        "    logger.info(f\"Ready for Google Cloud Vertex AI deployment\")\n",
        "    return model_path\n",
        "\n",
        "def load_nse_portfolio_model(model_path: str = \"nse_portfolio_model_v2\") -> RobustPortfolioManager:\n",
        "    logger.info(f\"Loading NSE portfolio model from {model_path}/...\")\n",
        "    portfolio_manager = RobustPortfolioManager()\n",
        "    portfolio_manager.model.model = tf.saved_model.load(f\"{model_path}/saved_model\")\n",
        "    logger.info(\"TensorFlow model loaded\")\n",
        "    portfolio_manager.model.scaler = joblib.load(f\"{model_path}/artifacts/scaler.pkl\")\n",
        "    portfolio_manager.feature_engineer.feature_names = joblib.load(f\"{model_path}/artifacts/feature_names.pkl\")\n",
        "    logger.info(\"Preprocessing components loaded\")\n",
        "\n",
        "    import json\n",
        "    with open(f\"{model_path}/model_metadata.json\", 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    portfolio_manager.model.sequence_length = metadata['feature_engineering']['sequence_length']\n",
        "    portfolio_manager.model.n_features = metadata['feature_engineering']['n_features']\n",
        "    portfolio_manager.model.is_trained = True\n",
        "    portfolio_manager.model_metadata = metadata['model_info']\n",
        "    portfolio_manager.training_history = metadata['training_history']\n",
        "\n",
        "    logger.info(f\"NSE Portfolio Model loaded successfully!\")\n",
        "    return portfolio_manager\n",
        "\n",
        "print(\"Model persistence system ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pSjqeK4dXf0",
        "outputId": "617a6bc2-8da1-4cdc-ebb7-549b6dd375f2"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model persistence system ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training execution for clean NSE stocks\n",
        "\n",
        "CLEAN_NSE_STOCKS = [\n",
        "    'JINDALSTEL.NS', 'JSWSTEEL.NS', 'TATASTEEL.NS', 'VEDL.NS', 'HINDALCO.NS',\n",
        "    'HINDZINC.NS', 'SAIL.NS', 'NMDC.NS',\n",
        "    'TATAMOTORS.NS', 'BAJAJ-AUTO.NS', 'EICHERMOT.NS', 'HEROMOTOCO.NS',\n",
        "    'M&M.NS', 'ASHOKLEY.NS', 'MARUTI.NS', 'TVSMOTOR.NS',\n",
        "    'BAJFINANCE.NS', 'MUTHOOTFIN.NS', 'ABFRL.NS', 'HDFCBANK.NS',\n",
        "    'ICICIBANK.NS', 'SHRIRAMFIN.NS', 'SBIN.NS', 'LICI.NS',\n",
        "    'JSWENERGY.NS', 'TATAPOWER.NS', 'ADANIPOWER.NS', 'RELIANCE.NS',\n",
        "    'POWERGRID.NS', 'IOC.NS', 'BPCL.NS', 'NTPC.NS',\n",
        "    'TCS.NS', 'WIPRO.NS', 'INFY.NS', 'TECHM.NS', 'HCLTECH.NS',\n",
        "    'PERSISTENT.NS', 'LT.NS', 'TATAELXSI.NS',\n",
        "    'BLUESTARCO.NS', 'VOLTAS.NS', 'CROMPTON.NS', 'HAVELLS.NS',\n",
        "    'BAJAJELEC.NS', 'WHIRLPOOL.NS', 'TITAN.NS', 'ASIANPAINT.NS',\n",
        "    'DABUR.NS', 'GODREJCP.NS', 'BRITANNIA.NS', 'ITC.NS',\n",
        "    'NESTLEIND.NS', 'MARICO.NS', 'TATACONSUM.NS', 'COLPAL.NS',\n",
        "    'CIPLA.NS', 'DRREDDY.NS', 'MANKIND.NS', 'SUNPHARMA.NS',\n",
        "    'LUPIN.NS', 'ZYDUSLIFE.NS', 'BIOCON.NS'\n",
        "]\n",
        "\n",
        "def run_final_nse_training():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"NSE PORTFOLIO MANAGER - FINAL TRAINING EXECUTION\")\n",
        "    print(\"All fixes applied: Multi-level columns, DataFrame/Series conversion\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        portfolio_manager = RobustPortfolioManager(\n",
        "            alpha_vantage_key=\"demo\",\n",
        "            fmp_key=\"demo\",\n",
        "            twelve_data_key=\"demo\"\n",
        "        )\n",
        "\n",
        "        print(f\"\\nTRAINING PHASE\")\n",
        "        print(f\"Training on {len(CLEAN_NSE_STOCKS)} verified NSE stocks\")\n",
        "        training_results = portfolio_manager.train_model(CLEAN_NSE_STOCKS)\n",
        "\n",
        "        print(f\"\\nTRAINING RESULTS:\")\n",
        "        print(f\"Status: {training_results['status'].upper()}\")\n",
        "        print(f\"Best Validation Accuracy: {training_results['best_val_accuracy']:.4f}\")\n",
        "        print(f\"Total Training Sequences: {training_results['model_metadata']['total_sequences']:,}\")\n",
        "        print(f\"Feature Count: {training_results['model_metadata']['feature_count']}\")\n",
        "        print(f\"Successful Symbols: {len(training_results['model_metadata']['trained_symbols'])}\")\n",
        "\n",
        "        print(f\"\\nPREDICTION PHASE\")\n",
        "        portfolio_results = portfolio_manager.analyze_portfolio(CLEAN_NSE_STOCKS)\n",
        "        summary = portfolio_results['summary']\n",
        "\n",
        "        print(f\"\\nPORTFOLIO ANALYSIS:\")\n",
        "        print(f\"Success Rate: {summary['successful_predictions']}/{summary['total_symbols']} ({summary['success_rate']:.1%})\")\n",
        "        print(f\"Average Confidence: {summary['average_confidence']:.2%}\")\n",
        "\n",
        "        print(f\"\\nACTION DISTRIBUTION:\")\n",
        "        total_predictions = summary['successful_predictions']\n",
        "        for action, count in summary['action_distribution'].items():\n",
        "            percentage = (count / total_predictions * 100) if total_predictions > 0 else 0\n",
        "            print(f\"   {action}: {count:>2} stocks ({percentage:>5.1f}%)\")\n",
        "\n",
        "        print(f\"\\nTOP 10 CONFIDENT PREDICTIONS:\")\n",
        "        confident_predictions = []\n",
        "        for symbol, result in portfolio_results['portfolio_analysis'].items():\n",
        "            if 'confidence' in result and 'action' in result:\n",
        "                confident_predictions.append((symbol, result['action'], result['confidence'], result['current_price']))\n",
        "\n",
        "        confident_predictions.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        for i, (symbol, action, confidence, price) in enumerate(confident_predictions[:10]):\n",
        "            print(f\"{i+1:>2}. {symbol:<15}: {action:<4} | {confidence:>6.2%} | Rs.{price:>8.2f}\")\n",
        "\n",
        "        print(f\"\\nSAVING MODEL...\")\n",
        "        model_path = save_nse_portfolio_model(portfolio_manager)\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 80)\n",
        "        print(\"NSE PORTFOLIO MANAGER - TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"Model saved to: {model_path}\")\n",
        "        print(f\"Validation Accuracy: {training_results['best_val_accuracy']:.1%}\")\n",
        "        print(f\"Portfolio Success Rate: {summary['success_rate']:.1%}\")\n",
        "        print(f\"Average Confidence: {summary['average_confidence']:.1%}\")\n",
        "        print(\"Ready for Google Cloud Vertex AI deployment\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'training_results': training_results,\n",
        "            'portfolio_analysis': portfolio_results,\n",
        "            'model_path': model_path\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTRAINING FAILED: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {'status': 'failed', 'error': str(e)}\n",
        "\n",
        "print(\"Starting NSE portfolio training with all fixes...\")\n",
        "final_results = run_final_nse_training()\n",
        "\n",
        "if final_results['status'] == 'success':\n",
        "    print(f\"\\nHACKATHON READY!\")\n",
        "    print(f\"Model path: {final_results['model_path']}\")\n",
        "else:\n",
        "    print(f\"\\nTraining failed: {final_results['error']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOhmdfBMdelb",
        "outputId": "85193e82-1b70-483a-a2fd-d8957b71f5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting NSE portfolio training with all fixes...\n",
            "================================================================================\n",
            "NSE PORTFOLIO MANAGER - FINAL TRAINING EXECUTION\n",
            "All fixes applied: Multi-level columns, DataFrame/Series conversion\n",
            "================================================================================\n",
            "\n",
            "TRAINING PHASE\n",
            "Training on 63 verified NSE stocks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Failed processing JINDALSTEL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing JSWSTEEL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TATASTEEL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing VEDL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing HINDALCO.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing HINDZINC.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing SAIL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing NMDC.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TATAMOTORS.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing BAJAJ-AUTO.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing EICHERMOT.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing HEROMOTOCO.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing M&M.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing ASHOKLEY.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing MARUTI.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TVSMOTOR.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing BAJFINANCE.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing MUTHOOTFIN.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing ABFRL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing HDFCBANK.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing ICICIBANK.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing SHRIRAMFIN.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing SBIN.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing LICI.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing JSWENERGY.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TATAPOWER.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing ADANIPOWER.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing RELIANCE.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing POWERGRID.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing IOC.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing BPCL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing NTPC.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TCS.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing WIPRO.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing INFY.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TECHM.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing HCLTECH.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing PERSISTENT.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing LT.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TATAELXSI.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing BLUESTARCO.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing VOLTAS.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing CROMPTON.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing HAVELLS.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing BAJAJELEC.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing WHIRLPOOL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TITAN.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing ASIANPAINT.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing DABUR.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing GODREJCP.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing BRITANNIA.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing NESTLEIND.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing MARICO.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing TATACONSUM.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing COLPAL.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing CIPLA.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing DRREDDY.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing MANKIND.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing SUNPHARMA.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing LUPIN.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing ZYDUSLIFE.NS: only length-1 arrays can be converted to Python scalars\n",
            "ERROR:__main__:Failed processing BIOCON.NS: only length-1 arrays can be converted to Python scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - accuracy: 0.3451 - loss: 1.6623 - val_accuracy: 0.8925 - val_loss: 0.7025 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.7926 - loss: 0.6924 - val_accuracy: 0.8925 - val_loss: 0.5088 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8429 - loss: 0.6151 - val_accuracy: 0.8925 - val_loss: 0.4730 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8562 - loss: 0.5560 - val_accuracy: 0.8925 - val_loss: 0.4343 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8562 - loss: 0.5772 - val_accuracy: 0.8925 - val_loss: 0.4351 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.8638 - loss: 0.5640 - val_accuracy: 0.8925 - val_loss: 0.4234 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8678 - loss: 0.5454 - val_accuracy: 0.8925 - val_loss: 0.4239 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8685 - loss: 0.5255 - val_accuracy: 0.8925 - val_loss: 0.4859 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8695 - loss: 0.5234 - val_accuracy: 0.8925 - val_loss: 0.4741 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8708 - loss: 0.5235 - val_accuracy: 0.8925 - val_loss: 0.4441 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8704 - loss: 0.5171 - val_accuracy: 0.8925 - val_loss: 0.4523 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.8714 - loss: 0.5022 - val_accuracy: 0.8925 - val_loss: 0.4162 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8708 - loss: 0.5058 - val_accuracy: 0.8925 - val_loss: 0.4337 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4898 - val_accuracy: 0.8925 - val_loss: 0.4262 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.5066 - val_accuracy: 0.8925 - val_loss: 0.4424 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.5065 - val_accuracy: 0.8925 - val_loss: 0.4570 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4880 - val_accuracy: 0.8925 - val_loss: 0.4922 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.8709 - loss: 0.4816 - val_accuracy: 0.8925 - val_loss: 0.4464 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8709 - loss: 0.4898 - val_accuracy: 0.8925 - val_loss: 0.4378 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4920 - val_accuracy: 0.8925 - val_loss: 0.4380 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4817 - val_accuracy: 0.8925 - val_loss: 0.4186 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4937 - val_accuracy: 0.8925 - val_loss: 0.4361 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4890 - val_accuracy: 0.8925 - val_loss: 0.4720 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8709 - loss: 0.4801 - val_accuracy: 0.8925 - val_loss: 0.5302 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8709 - loss: 0.4784 - val_accuracy: 0.8925 - val_loss: 0.4206 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4690 - val_accuracy: 0.8925 - val_loss: 0.4617 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4710 - val_accuracy: 0.8925 - val_loss: 0.4136 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4645 - val_accuracy: 0.8925 - val_loss: 0.4165 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4819 - val_accuracy: 0.8925 - val_loss: 0.4218 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.8709 - loss: 0.4770 - val_accuracy: 0.8925 - val_loss: 0.4168 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4669 - val_accuracy: 0.8925 - val_loss: 0.4947 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4678 - val_accuracy: 0.8925 - val_loss: 0.4217 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4729 - val_accuracy: 0.8925 - val_loss: 0.4198 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8709 - loss: 0.4626 - val_accuracy: 0.8925 - val_loss: 0.4167 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8709 - loss: 0.4615 - val_accuracy: 0.8925 - val_loss: 0.4230 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4603 - val_accuracy: 0.8925 - val_loss: 0.4229 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4621 - val_accuracy: 0.8925 - val_loss: 0.4387 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4581 - val_accuracy: 0.8925 - val_loss: 0.4238 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4587 - val_accuracy: 0.8925 - val_loss: 0.5100 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4649 - val_accuracy: 0.8925 - val_loss: 0.5232 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8709 - loss: 0.4617 - val_accuracy: 0.8925 - val_loss: 0.4562 - learning_rate: 0.0010\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8709 - loss: 0.4649 - val_accuracy: 0.8925 - val_loss: 0.4512 - learning_rate: 0.0010\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4455 - val_accuracy: 0.8925 - val_loss: 0.4602 - learning_rate: 0.0010\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4553 - val_accuracy: 0.8925 - val_loss: 0.4659 - learning_rate: 0.0010\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4564 - val_accuracy: 0.8925 - val_loss: 0.4803 - learning_rate: 0.0010\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4610 - val_accuracy: 0.8925 - val_loss: 0.4157 - learning_rate: 0.0010\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4632 - val_accuracy: 0.8925 - val_loss: 0.4168 - learning_rate: 0.0010\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8709 - loss: 0.4556 - val_accuracy: 0.8925 - val_loss: 0.4228 - learning_rate: 5.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4524 - val_accuracy: 0.8925 - val_loss: 0.4181 - learning_rate: 5.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4574 - val_accuracy: 0.8925 - val_loss: 0.4287 - learning_rate: 5.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4529 - val_accuracy: 0.8925 - val_loss: 0.4202 - learning_rate: 5.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4455 - val_accuracy: 0.8925 - val_loss: 0.5059 - learning_rate: 5.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8709 - loss: 0.4519 - val_accuracy: 0.8925 - val_loss: 0.4413 - learning_rate: 5.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8709 - loss: 0.4503 - val_accuracy: 0.8925 - val_loss: 0.4664 - learning_rate: 5.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4515 - val_accuracy: 0.8925 - val_loss: 0.4218 - learning_rate: 5.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4564 - val_accuracy: 0.8925 - val_loss: 0.4909 - learning_rate: 5.0000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4471 - val_accuracy: 0.8925 - val_loss: 0.4670 - learning_rate: 5.0000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4487 - val_accuracy: 0.8925 - val_loss: 0.4346 - learning_rate: 5.0000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.8709 - loss: 0.4469 - val_accuracy: 0.8925 - val_loss: 0.4193 - learning_rate: 5.0000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4468 - val_accuracy: 0.8925 - val_loss: 0.4195 - learning_rate: 5.0000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4541 - val_accuracy: 0.8925 - val_loss: 0.4816 - learning_rate: 5.0000e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4454 - val_accuracy: 0.8925 - val_loss: 0.4296 - learning_rate: 5.0000e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4473 - val_accuracy: 0.8925 - val_loss: 0.5066 - learning_rate: 5.0000e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4482 - val_accuracy: 0.8925 - val_loss: 0.4753 - learning_rate: 5.0000e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8709 - loss: 0.4520 - val_accuracy: 0.8925 - val_loss: 0.4253 - learning_rate: 5.0000e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4482 - val_accuracy: 0.8925 - val_loss: 0.4488 - learning_rate: 5.0000e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4455 - val_accuracy: 0.8925 - val_loss: 0.4483 - learning_rate: 5.0000e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4467 - val_accuracy: 0.8925 - val_loss: 0.4600 - learning_rate: 2.5000e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4385 - val_accuracy: 0.8925 - val_loss: 0.4303 - learning_rate: 2.5000e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4386 - val_accuracy: 0.8925 - val_loss: 0.4205 - learning_rate: 2.5000e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4432 - val_accuracy: 0.8925 - val_loss: 0.4356 - learning_rate: 2.5000e-04\n",
            "Epoch 72/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_portfolio_demo():\n",
        "    print(\" NSE PORTFOLIO MANAGER - LIVE DEMO\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    demo_stocks = ['RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS', 'INFY.NS', 'TATASTEEL.NS']\n",
        "\n",
        "    if 'final_results' in globals() and final_results['status'] == 'success':\n",
        "        loaded_manager = load_nse_portfolio_model(final_results['model_path'])\n",
        "\n",
        "        print(f\"\\n INDIVIDUAL STOCK PREDICTIONS:\")\n",
        "        for stock in demo_stocks:\n",
        "            try:\n",
        "                prediction = loaded_manager.predict_stock(stock)\n",
        "                print(f\"{stock:<15}: {prediction.action:<4} | \"\n",
        "                      f\"Confidence: {prediction.confidence:>6.2%} | \"\n",
        "                      f\"Price: {prediction.current_price:>8.2f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"{stock:<15}: ERROR - {str(e)[:40]}...\")\n",
        "\n",
        "        training_summary = final_results['training_results']\n",
        "        portfolio_summary = final_results['portfolio_analysis']['summary']\n",
        "\n",
        "        print(f\"\\n MODEL PERFORMANCE:\")\n",
        "        print(f\"Validation Accuracy: {training_summary['best_val_accuracy']:>6.1%}\")\n",
        "        print(f\"Portfolio Success Rate: {portfolio_summary['success_rate']:>6.1%}\")\n",
        "        print(f\"Average Confidence: {portfolio_summary['average_confidence']:>6.1%}\")\n",
        "        print(f\"Model Features: {training_summary['model_metadata']['feature_count']}\")\n",
        "        print(f\"Training Sequences: {training_summary['model_metadata']['total_sequences']:,}\")\n",
        "    else:\n",
        "        print(\" No trained model available\")\n",
        "\n",
        "run_portfolio_demo()\n"
      ],
      "metadata": {
        "id": "dHyyZCAWvnlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}