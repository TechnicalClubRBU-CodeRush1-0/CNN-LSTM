{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1utovBvIb8r0atq8bo9Ygn8zPw_HNW7gg",
      "authorship_tag": "ABX9TyN4qctlaNVZyS6TSm3jamsR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechnicalClubRBU-CodeRush1-0/CNN-LSTM/blob/main/CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment setup and library imports for CNN-LSTM portfolio manager\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import requests\n",
        "import warnings\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Intelligent Portfolio Manager Initializing...\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "gpu_count = len(tf.config.list_physical_devices('GPU'))\n",
        "print(f\"GPU acceleration: {'Enabled' if gpu_count > 0 else 'CPU mode'} ({gpu_count} devices)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP3p2gYoG0zi",
        "outputId": "646580fb-400d-49ef-cd9f-b56abb548b93"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intelligent Portfolio Manager Initializing...\n",
            "TensorFlow version: 2.19.0\n",
            "GPU acceleration: Enabled (1 devices)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core data structures for predictions and market data\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    symbol: str\n",
        "    action: str\n",
        "    confidence: float\n",
        "    current_price: float\n",
        "    technical_indicators: Dict[str, float]\n",
        "    data_source: str\n",
        "    timestamp: str\n",
        "    model_version: str = \"1.0.0\"\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'symbol': self.symbol,\n",
        "            'action': self.action,\n",
        "            'confidence': round(self.confidence, 4),\n",
        "            'current_price': round(self.current_price, 2),\n",
        "            'technical_indicators': {k: round(v, 4) for k, v in self.technical_indicators.items()},\n",
        "            'data_source': self.data_source,\n",
        "            'timestamp': self.timestamp,\n",
        "            'model_version': self.model_version\n",
        "        }\n",
        "\n",
        "@dataclass\n",
        "class MarketData:\n",
        "    symbol: str\n",
        "    data: pd.DataFrame\n",
        "    source: str\n",
        "    timestamp: str\n",
        "    is_mock: bool = False\n",
        "\n",
        "print(\"Core data structures configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGJkwX9jG7B6",
        "outputId": "12df5c00-4bd9-4813-c301-f8cd39879d23"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core data structures configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced data manager with NSE fix\n",
        "\n",
        "class RobustDataManager:\n",
        "    def __init__(self, alpha_vantage_key: Optional[str] = None, fmp_key: Optional[str] = None,\n",
        "                 twelve_data_key: Optional[str] = None):\n",
        "        self.alpha_vantage_key = alpha_vantage_key or \"demo\"\n",
        "        self.fmp_key = fmp_key or \"demo\"\n",
        "        self.twelve_data_key = twelve_data_key or \"demo\"\n",
        "        self.data_sources = ['yfinance', 'alpha_vantage', 'fmp']\n",
        "        self.cache = {}\n",
        "\n",
        "    def fetch_yfinance(self, symbol: str, period: int = 120) -> Optional[pd.DataFrame]:\n",
        "        try:\n",
        "            logger.info(f\"Fetching {symbol} from YFinance...\")\n",
        "            data = yf.download(symbol, period=f'{period}d', progress=False, threads=True)\n",
        "\n",
        "            if data.empty:\n",
        "                raise ValueError(\"YFinance returned empty dataset\")\n",
        "\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "            if 'Adj Close' in data.columns:\n",
        "                data = data.drop('Adj Close', axis=1)\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"YFinance failed for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_alpha_vantage(self, symbol: str) -> Optional[pd.DataFrame]:\n",
        "        try:\n",
        "            logger.info(f\"Fetching {symbol} from Alpha Vantage...\")\n",
        "            base_symbol = symbol.replace('.NS', '')\n",
        "            url = \"https://www.alphavantage.co/query\"\n",
        "            params = {\n",
        "                'function': 'TIME_SERIES_DAILY',\n",
        "                'symbol': f'{base_symbol}.BSE',\n",
        "                'apikey': self.alpha_vantage_key,\n",
        "                'outputsize': 'full'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=15)\n",
        "            data = response.json()\n",
        "\n",
        "            if 'Time Series (Daily)' not in data:\n",
        "                raise ValueError(f\"Alpha Vantage API error: {data.get('Note', 'Unknown error')}\")\n",
        "\n",
        "            time_series = data['Time Series (Daily)']\n",
        "            df = pd.DataFrame.from_dict(time_series, orient='index')\n",
        "            df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df.astype(float).sort_index()\n",
        "\n",
        "            return df.tail(120)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Alpha Vantage failed for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_fmp(self, symbol: str) -> Optional[pd.DataFrame]:\n",
        "        try:\n",
        "            logger.info(f\"Fetching {symbol} from Financial Modeling Prep...\")\n",
        "            base_symbol = symbol.replace('.NS', '.BSE')\n",
        "            url = f\"https://financialmodelingprep.com/api/v3/historical-price-full/{base_symbol}\"\n",
        "            params = {\n",
        "                'apikey': self.fmp_key,\n",
        "                'from': (datetime.now() - timedelta(days=130)).strftime('%Y-%m-%d'),\n",
        "                'to': datetime.now().strftime('%Y-%m-%d')\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=15)\n",
        "            data = response.json()\n",
        "\n",
        "            if 'historical' not in data or not data['historical']:\n",
        "                raise ValueError(\"FMP returned no historical data\")\n",
        "\n",
        "            df = pd.DataFrame(data['historical'])\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.set_index('date').sort_index()\n",
        "\n",
        "            column_mapping = {\n",
        "                'open': 'Open', 'high': 'High', 'low': 'Low',\n",
        "                'close': 'Close', 'volume': 'Volume'\n",
        "            }\n",
        "            df = df.rename(columns=column_mapping)\n",
        "\n",
        "            return df[['Open', 'High', 'Low', 'Close', 'Volume']].tail(120)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"FMP failed for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_market_data(self, symbol: str) -> MarketData:\n",
        "        active_sources = ['yfinance', 'alpha_vantage', 'fmp']\n",
        "\n",
        "        for source in active_sources:\n",
        "            try:\n",
        "                data = None\n",
        "                if source == 'yfinance':\n",
        "                    data = self.fetch_yfinance(symbol)\n",
        "                elif source == 'alpha_vantage':\n",
        "                    data = self.fetch_alpha_vantage(symbol)\n",
        "                elif source == 'fmp':\n",
        "                    data = self.fetch_fmp(symbol)\n",
        "\n",
        "                if data is not None and not data.empty and len(data) >= 60:\n",
        "                    market_data = MarketData(\n",
        "                        symbol=symbol,\n",
        "                        data=data,\n",
        "                        source=source,\n",
        "                        timestamp=datetime.now().isoformat(),\n",
        "                        is_mock=False\n",
        "                    )\n",
        "\n",
        "                    logger.info(f\"Successfully fetched {symbol} from {source} ({len(data)} rows)\")\n",
        "                    return market_data\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error with {source} for {symbol}: {e}\")\n",
        "                continue\n",
        "\n",
        "        raise RuntimeError(f\"All data sources failed for {symbol}\")\n",
        "\n",
        "print(\"Data manager configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0GGBswAcKgZ",
        "outputId": "33f89c3f-a522-4e13-a0c1-690d1c3fb89a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data manager configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed advanced feature engineering with proper Series handling\n",
        "\n",
        "# Fixed advanced feature engineering with proper Series handling and correct indentation\n",
        "\n",
        "class AdvancedFeatureEngineer:\n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "\n",
        "    def _ensure_series(self, result, name=\"calculation\"):\n",
        "        if isinstance(result, pd.DataFrame):\n",
        "            if len(result.columns) == 1:\n",
        "                return result.iloc[:, 0]\n",
        "            else:\n",
        "                logger.warning(f\"{name} returned DataFrame with multiple columns, using first column\")\n",
        "                return result.iloc[:, 0]\n",
        "        return result\n",
        "\n",
        "    def add_price_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        df['Returns'] = df['Close'].pct_change()\n",
        "        df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "        df['Price_Range'] = (df['High'] - df['Low']) / df['Close']\n",
        "        df['Gap'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)\n",
        "        df['Body_Size'] = abs(df['Close'] - df['Open']) / df['Close']\n",
        "        df['Upper_Shadow'] = (df['High'] - np.maximum(df['Open'], df['Close'])) / df['Close']\n",
        "        df['Lower_Shadow'] = (np.minimum(df['Open'], df['Close']) - df['Low']) / df['Close']\n",
        "        return df\n",
        "\n",
        "    def add_moving_averages(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        periods = [3, 5, 8, 13, 21, 34, 55]\n",
        "        for period in periods:\n",
        "            sma_col = df['Close'].rolling(window=period).mean()\n",
        "            sma_col = self._ensure_series(sma_col, f\"SMA_{period}\")\n",
        "            df[f'SMA_{period}'] = sma_col\n",
        "            ratio_calc = df['Close'] / sma_col\n",
        "            ratio_calc = self._ensure_series(ratio_calc, f\"SMA_{period}_Ratio\")\n",
        "            df[f'SMA_{period}_Ratio'] = ratio_calc\n",
        "            slope_calc = sma_col.diff(5) / sma_col\n",
        "            slope_calc = self._ensure_series(slope_calc, f\"SMA_{period}_Slope\")\n",
        "            df[f'SMA_{period}_Slope'] = slope_calc\n",
        "\n",
        "        ema_periods = [8, 13, 21, 34, 55]\n",
        "        for period in ema_periods:\n",
        "            ema_col = df['Close'].ewm(span=period).mean()\n",
        "            ema_col = self._ensure_series(ema_col, f\"EMA_{period}\")\n",
        "            df[f'EMA_{period}'] = ema_col\n",
        "            ratio_calc = df['Close'] / ema_col\n",
        "            ratio_calc = self._ensure_series(ratio_calc, f\"EMA_{period}_Ratio\")\n",
        "            df[f'EMA_{period}_Ratio'] = ratio_calc\n",
        "        return df\n",
        "\n",
        "    def add_momentum_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        delta = df['Close'].diff()\n",
        "        gain = delta.where(delta > 0, 0).rolling(window=21).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=21).mean()\n",
        "        gain = self._ensure_series(gain, \"RSI_gain\")\n",
        "        loss = self._ensure_series(loss, \"RSI_loss\")\n",
        "        rs = gain / loss\n",
        "        rs = self._ensure_series(rs, \"RS\")\n",
        "        df['RSI'] = 100 - (100 / (1 + rs))\n",
        "        df['RSI_Momentum'] = df['RSI'].diff(5)\n",
        "\n",
        "        ema_12 = self._ensure_series(df['Close'].ewm(span=12).mean(), \"EMA_12\")\n",
        "        ema_26 = self._ensure_series(df['Close'].ewm(span=26).mean(), \"EMA_26\")\n",
        "        df['EMA_12'] = ema_12\n",
        "        df['EMA_26'] = ema_26\n",
        "        df['MACD'] = ema_12 - ema_26\n",
        "        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "        df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "        df['MACD_Slope'] = df['MACD'].diff(3)\n",
        "\n",
        "        low_21 = self._ensure_series(df['Low'].rolling(21).min(), \"Low_21\")\n",
        "        high_21 = self._ensure_series(df['High'].rolling(21).max(), \"High_21\")\n",
        "        df['Stoch_K'] = 100 * ((df['Close'] - low_21) / (high_21 - low_21))\n",
        "        df['Stoch_D'] = df['Stoch_K'].rolling(5).mean()\n",
        "        df['Williams_R'] = -100 * ((high_21 - df['Close']) / (high_21 - low_21))\n",
        "        df['ROC'] = ((df['Close'] - df['Close'].shift(13)) / df['Close'].shift(13)) * 100\n",
        "        df['Momentum'] = df['Close'] - df['Close'].shift(13)\n",
        "        return df\n",
        "\n",
        "    def add_volatility_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "\n",
        "        # Bollinger Bands & related\n",
        "        bb_middle = df['Close'].rolling(21).mean()\n",
        "        bb_std = df['Close'].rolling(21).std()\n",
        "        df['BB_Middle'] = bb_middle\n",
        "        df['BB_Upper'] = bb_middle + 2.5*bb_std\n",
        "        df['BB_Lower'] = bb_middle - 2.5*bb_std\n",
        "        df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / bb_middle\n",
        "        df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "        df['BB_Squeeze'] = bb_std / bb_middle\n",
        "\n",
        "        # True Range & ATR\n",
        "        tr1 = df['High'] - df['Low']\n",
        "        tr2 = (df['High'] - df['Close'].shift(1)).abs()\n",
        "        tr3 = (df['Low'] - df['Close'].shift(1)).abs()\n",
        "        df['TR'] = np.vstack([tr1, tr2, tr3]).max(axis=0)\n",
        "\n",
        "        atr = df['TR'].rolling(21).mean()\n",
        "        df['ATR'] = atr\n",
        "        df['ATR_Ratio'] = atr / df['Close']\n",
        "\n",
        "        # Realised volatility metrics\n",
        "        returns = df['Returns']\n",
        "        df['Volatility_5'] = returns.rolling(5).std()*np.sqrt(252)\n",
        "        df['Volatility_13'] = returns.rolling(13).std()*np.sqrt(252)\n",
        "        df['Volatility_21'] = returns.rolling(21).std()*np.sqrt(252)\n",
        "        df['Volatility_Ratio'] = df['Volatility_5'] / df['Volatility_21']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_volume_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        vol_sma_5 = self._ensure_series(df['Volume'].rolling(5).mean(), \"Volume_SMA_5\")\n",
        "        vol_sma_13 = self._ensure_series(df['Volume'].rolling(13).mean(), \"Volume_SMA_13\")\n",
        "        vol_sma_21 = self._ensure_series(df['Volume'].rolling(21).mean(), \"Volume_SMA_21\")\n",
        "        df['Volume_SMA_5'] = vol_sma_5\n",
        "        df['Volume_SMA_13'] = vol_sma_13\n",
        "        df['Volume_SMA_21'] = vol_sma_21\n",
        "        df['Volume_Ratio_5'] = self._ensure_series(df['Volume'] / vol_sma_5, \"Volume_Ratio_5\")\n",
        "        df['Volume_Ratio_21'] = self._ensure_series(df['Volume'] / vol_sma_21, \"Volume_Ratio_21\")\n",
        "        df['Volume_Price_Trend'] = (df['Volume'] * df['Returns']).cumsum()\n",
        "        df['Price_Volume'] = df['Close'] * df['Volume']\n",
        "        volume_price_sum = (df['Close'] * df['Volume']).rolling(13).sum()\n",
        "        volume_sum = df['Volume'].rolling(13).sum()\n",
        "        vwp_calc = volume_price_sum / volume_sum\n",
        "        df['Volume_Weighted_Price'] = self._ensure_series(vwp_calc, \"Volume_Weighted_Price\")\n",
        "        return df\n",
        "\n",
        "    def create_sequences(self, df: pd.DataFrame, sequence_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        feature_cols = [\n",
        "            'Returns', 'Log_Returns', 'Price_Range', 'Body_Size', 'Upper_Shadow', 'Lower_Shadow',\n",
        "            'SMA_3_Ratio', 'SMA_5_Ratio', 'SMA_8_Ratio', 'SMA_13_Ratio', 'SMA_21_Ratio',\n",
        "            'EMA_8_Ratio', 'EMA_13_Ratio', 'EMA_21_Ratio', 'EMA_34_Ratio',\n",
        "            'RSI', 'RSI_Momentum', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'MACD_Slope',\n",
        "            'Stoch_K', 'Stoch_D', 'Williams_R', 'ROC', 'Momentum',\n",
        "            'BB_Position', 'BB_Width', 'BB_Squeeze', 'ATR_Ratio',\n",
        "            'Volatility_5', 'Volatility_13', 'Volatility_21', 'Volatility_Ratio',\n",
        "            'Volume_Ratio_5', 'Volume_Ratio_21', 'Volume_Price_Trend'\n",
        "        ]\n",
        "\n",
        "        available_cols = [col for col in feature_cols if col in df.columns]\n",
        "        self.feature_names = available_cols\n",
        "        features = df[available_cols].fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "        X, y = [], []\n",
        "\n",
        "        assert all(features[col].ndim == 1 for col in features.columns), \"Feature engineering produced multidimensional column\"\n",
        "\n",
        "        for i in range(sequence_length, len(features) - 1):\n",
        "            X.append(features.iloc[i-sequence_length:i].values)\n",
        "            current_price = df['Close'].iloc[i]\n",
        "            future_price = df['Close'].iloc[i+1]\n",
        "            if pd.isna(current_price) or pd.isna(future_price) or current_price == 0:\n",
        "                continue\n",
        "            future_return = (future_price - current_price) / current_price\n",
        "            if future_return > 0.02:\n",
        "                label = 2\n",
        "            elif future_return < -0.02:\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "            y.append(label)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y, dtype=np.int32)\n",
        "        valid_indices = ~(np.isnan(X).any(axis=(1,2)) | np.isnan(y))\n",
        "        X = X[valid_indices]\n",
        "        y = y[valid_indices]\n",
        "        logger.info(f\"Created {len(X)} valid sequences\")\n",
        "        logger.info(f\"Label distribution: SELL:{np.sum(y==0)}, HOLD:{np.sum(y==1)}, BUY:{np.sum(y==2)}\")\n",
        "        return X, y\n",
        "\n",
        "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        try:\n",
        "            df = self.add_price_features(df)\n",
        "            df = self.add_moving_averages(df)\n",
        "            df = self.add_momentum_indicators(df)\n",
        "            df = self.add_volatility_indicators(df)\n",
        "            df = self.add_volume_indicators(df)\n",
        "            df = df.fillna(method='bfill').fillna(method='ffill')\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Feature engineering failed: {e}\")\n",
        "            raise\n",
        "\n",
        "print(\"Feature engineering configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcGckyyBcxrI",
        "outputId": "64191347-23ee-4664-b246-0925f0ca12e2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineering configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLSTMModel:\n",
        "    def __init__(self, sequence_length: int = 60, n_features: int = 35):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.n_features = n_features\n",
        "        self.model = None\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.is_trained = False\n",
        "        self.history = None\n",
        "\n",
        "    def build_model(self) -> tf.keras.Model:\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu',\n",
        "                                  input_shape=(self.sequence_length, self.n_features)),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.3),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.LSTM(25, dropout=0.3),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(3, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def validate_data(self, X: np.ndarray, y: np.ndarray):\n",
        "        logger.info(\"Validating training data...\")\n",
        "        if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
        "            logger.error(\"Found NaN or infinite values in X\")\n",
        "            raise ValueError(\"Training data X contains NaN or infinite values\")\n",
        "        unique_labels = np.unique(y)\n",
        "        valid_labels = {0, 1, 2}\n",
        "        logger.info(f\"Unique labels found: {unique_labels}\")\n",
        "        logger.info(f\"Label distribution: {np.bincount(y)}\")\n",
        "        if not set(unique_labels).issubset(valid_labels):\n",
        "            invalid_labels = set(unique_labels) - valid_labels\n",
        "            logger.error(f\"Invalid labels found: {invalid_labels}\")\n",
        "            raise ValueError(f\"Labels must be 0, 1, or 2. Found invalid labels: {invalid_labels}\")\n",
        "        if np.any(np.isnan(y)):\n",
        "            logger.error(\"Found NaN values in labels y\")\n",
        "            raise ValueError(\"Labels y contain NaN values\")\n",
        "        logger.info(\"Data validation passed!\")\n",
        "\n",
        "    def train(self, X: np.ndarray, y: np.ndarray, validation_split: float = 0.25):\n",
        "        logger.info(f\"Training CNN-LSTM model on {len(X)} sequences...\")\n",
        "        logger.info(f\"Feature dimensions: {X.shape}\")\n",
        "        self.validate_data(X, y)\n",
        "        X_scaled = self.scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        y = y.astype(np.int32)\n",
        "        self.model = self.build_model()\n",
        "        logger.info(f\"Model parameters: {self.model.count_params():,}\")\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                patience=50,\n",
        "                restore_best_weights=True,\n",
        "                monitor='val_loss',\n",
        "                mode='min'\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                patience=20,\n",
        "                factor=0.5,\n",
        "                min_lr=1e-6,\n",
        "                monitor='val_loss'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_scaled, y,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        self.is_trained = True\n",
        "        best_val_acc = max(self.history.history['val_accuracy'])\n",
        "        final_loss = self.history.history['loss'][-1]\n",
        "        logger.info(f\"Training completed!\")\n",
        "        logger.info(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "        logger.info(f\"Final loss: {final_loss:.4f}\")\n",
        "        return self.history\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "        X_scaled = self.scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "        predictions = self.model.predict(X_scaled, verbose=0)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "        confidence_scores = np.max(predictions, axis=1)\n",
        "        return predicted_classes, confidence_scores\n",
        "\n",
        "print(\"CNN-LSTM model configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG8u1de7cz_8",
        "outputId": "c3105e06-2c42-47c8-ecb0-c9ab30b79b2e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN-LSTM model configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main portfolio management system integrating all fixed components for NSE stock analysis\n",
        "\n",
        "class RobustPortfolioManager:\n",
        "    def __init__(self, alpha_vantage_key: Optional[str] = None,\n",
        "                 fmp_key: Optional[str] = None,\n",
        "                 twelve_data_key: Optional[str] = None):\n",
        "        self.data_manager = RobustDataManager(alpha_vantage_key, fmp_key, twelve_data_key)\n",
        "        self.feature_engineer = AdvancedFeatureEngineer()\n",
        "        self.model = None  # Will be initialized during training with correct feature count\n",
        "        self.action_map = {0: 'SELL', 1: 'HOLD', 2: 'BUY'}\n",
        "        self.model_metadata = {}\n",
        "        self.training_history = {}\n",
        "\n",
        "    def train_model(self, symbols: List[str]) -> Dict:\n",
        "        \"\"\"Train the CNN-LSTM model on multiple stock symbols\"\"\"\n",
        "        logger.info(f\"Starting training on {len(symbols)} symbols...\")\n",
        "\n",
        "        all_X = []\n",
        "        all_y = []\n",
        "        successful_symbols = []\n",
        "        failed_symbols = {}\n",
        "        n_features = None  # Will be determined from first successful symbol\n",
        "\n",
        "        for symbol in symbols:\n",
        "            try:\n",
        "                logger.info(f\"Processing {symbol} for training...\")\n",
        "                market_data = self.data_manager.get_market_data(symbol)\n",
        "\n",
        "                # Engineer features\n",
        "                df_features = self.feature_engineer.engineer_features(market_data.data)\n",
        "\n",
        "                # Create sequences\n",
        "                X, y = self.feature_engineer.create_sequences(df_features)\n",
        "\n",
        "                if len(X) > 0:\n",
        "                    # Determine feature count from first successful symbol\n",
        "                    if n_features is None:\n",
        "                        n_features = X.shape[2]\n",
        "                        logger.info(f\"Detected {n_features} features from data\")\n",
        "\n",
        "                    all_X.append(X)\n",
        "                    all_y.append(y)\n",
        "                    successful_symbols.append(symbol)\n",
        "                    logger.info(f\"Added {len(X)} sequences from {symbol}\")\n",
        "                else:\n",
        "                    failed_symbols[symbol] = \"No valid sequences created\"\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to process {symbol}: {e}\")\n",
        "                failed_symbols[symbol] = str(e)\n",
        "                continue\n",
        "\n",
        "        if not all_X:\n",
        "            raise ValueError(\"No training data could be collected from any symbol\")\n",
        "\n",
        "        # Combine all sequences\n",
        "        X_train = np.concatenate(all_X, axis=0)\n",
        "        y_train = np.concatenate(all_y, axis=0)\n",
        "\n",
        "        logger.info(f\"Total training sequences: {len(X_train)}\")\n",
        "        logger.info(f\"Feature dimensions: {X_train.shape}\")\n",
        "\n",
        "        # Initialize model with correct feature count\n",
        "        self.model = CNNLSTMModel(\n",
        "            sequence_length=60,\n",
        "            n_features=n_features  # Use actual feature count\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        history = self.model.train(X_train, y_train)\n",
        "\n",
        "        # Store metadata\n",
        "        self.model_metadata = {\n",
        "            'trained_symbols': successful_symbols,\n",
        "            'failed_symbols': failed_symbols,\n",
        "            'total_sequences': len(X_train),\n",
        "            'feature_count': X_train.shape[2],\n",
        "            'training_date': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self.training_history = {\n",
        "            'loss': history.history['loss'],\n",
        "            'accuracy': history.history['accuracy'],\n",
        "            'val_loss': history.history['val_loss'],\n",
        "            'val_accuracy': history.history['val_accuracy']\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'best_val_accuracy': max(history.history['val_accuracy']),\n",
        "            'final_loss': history.history['loss'][-1],\n",
        "            'model_metadata': self.model_metadata\n",
        "        }\n",
        "\n",
        "    def predict_stock(self, symbol: str) -> PredictionResult:\n",
        "        \"\"\"Make prediction for a single stock\"\"\"\n",
        "        if self.model is None or not self.model.is_trained:\n",
        "            raise ValueError(\"Model must be trained before making predictions\")\n",
        "\n",
        "        try:\n",
        "            # Get market data\n",
        "            market_data = self.data_manager.get_market_data(symbol)\n",
        "\n",
        "            # Engineer features\n",
        "            df_features = self.feature_engineer.engineer_features(market_data.data)\n",
        "\n",
        "            # Create sequences for prediction\n",
        "            X, _ = self.feature_engineer.create_sequences(df_features)\n",
        "\n",
        "            if len(X) == 0:\n",
        "                raise ValueError(\"No valid sequences for prediction\")\n",
        "\n",
        "            # Use last sequence for current prediction\n",
        "            X_last = X[-1:, :, :]\n",
        "\n",
        "            # Get prediction\n",
        "            predictions, confidences = self.model.predict(X_last)\n",
        "\n",
        "            # Get current technical indicators\n",
        "            last_row = df_features.iloc[-1]\n",
        "            technical_indicators = {\n",
        "                'RSI': float(last_row.get('RSI', 0)),\n",
        "                'MACD': float(last_row.get('MACD', 0)),\n",
        "                'BB_Position': float(last_row.get('BB_Position', 0)),\n",
        "                'Volume_Ratio_5': float(last_row.get('Volume_Ratio_5', 0))\n",
        "            }\n",
        "\n",
        "            result = PredictionResult(\n",
        "                symbol=symbol,\n",
        "                action=self.action_map[predictions[0]],\n",
        "                confidence=float(confidences[0]),\n",
        "                current_price=float(market_data.data['Close'].iloc[-1]),\n",
        "                technical_indicators=technical_indicators,\n",
        "                data_source=market_data.source,\n",
        "                timestamp=datetime.now().isoformat()\n",
        "            )\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Prediction failed for {symbol}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def analyze_portfolio(self, symbols: List[str]) -> Dict:\n",
        "        \"\"\"Analyze multiple stocks and provide portfolio recommendations\"\"\"\n",
        "        logger.info(f\"Analyzing portfolio of {len(symbols)} stocks...\")\n",
        "\n",
        "        portfolio_analysis = {}\n",
        "        successful_predictions = 0\n",
        "        failed_predictions = []\n",
        "        action_counts = {'BUY': 0, 'HOLD': 0, 'SELL': 0}\n",
        "        confidence_scores = []\n",
        "\n",
        "        for symbol in symbols:\n",
        "            try:\n",
        "                prediction = self.predict_stock(symbol)\n",
        "                portfolio_analysis[symbol] = prediction.to_dict()\n",
        "                successful_predictions += 1\n",
        "                action_counts[prediction.action] += 1\n",
        "                confidence_scores.append(prediction.confidence)\n",
        "                logger.info(f\"{symbol}: {prediction.action} (confidence: {prediction.confidence:.2%})\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to analyze {symbol}: {e}\")\n",
        "                portfolio_analysis[symbol] = {\n",
        "                    'error': str(e),\n",
        "                    'data_source': 'Error',\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                failed_predictions.append((symbol, str(e)))\n",
        "                continue\n",
        "\n",
        "        summary = {\n",
        "            'total_symbols': len(symbols),\n",
        "            'successful_predictions': successful_predictions,\n",
        "            'failed_predictions_count': len(failed_predictions),\n",
        "            'failed_predictions_details': {sym: reason for sym, reason in failed_predictions},\n",
        "            'success_rate': successful_predictions / len(symbols),\n",
        "            'action_distribution': action_counts,\n",
        "            'average_confidence': np.mean(confidence_scores) if confidence_scores else 0,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'portfolio_analysis': portfolio_analysis,\n",
        "            'summary': summary\n",
        "        }\n",
        "\n",
        "print(\"RobustPortfolioManager configured with dynamic feature detection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHI1SrxdUEv",
        "outputId": "24722e6b-f98f-47bc-f5b9-afa8e310831a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobustPortfolioManager configured with dynamic feature detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model persistence and utility functions for deployment\n",
        "\n",
        "def save_nse_portfolio_model(portfolio_manager: RobustPortfolioManager,\n",
        "                             model_path: str = \"nse_portfolio_model_v2\") -> str:\n",
        "    \"\"\"Save model in multiple formats for maximum compatibility with Colab and Vertex AI\"\"\"\n",
        "\n",
        "    if not portfolio_manager.model.is_trained:\n",
        "        raise ValueError(\"Cannot save untrained model!\")\n",
        "\n",
        "    logger.info(f\"Saving NSE portfolio model to {model_path}/...\")\n",
        "\n",
        "    # Clean up any existing directory\n",
        "    if os.path.exists(model_path):\n",
        "        shutil.rmtree(model_path)\n",
        "\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    os.makedirs(f\"{model_path}/artifacts\", exist_ok=True)\n",
        "\n",
        "    # Save in multiple formats for compatibility\n",
        "    try:\n",
        "        # 1. Save as Keras native format (.keras)\n",
        "        keras_path = f\"{model_path}/model.keras\"\n",
        "        portfolio_manager.model.model.save(keras_path)\n",
        "        logger.info(f\"✓ Keras model saved: {keras_path}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not save .keras format: {e}\")\n",
        "\n",
        "    try:\n",
        "        # 2. Export as SavedModel format for TensorFlow Serving/Vertex AI\n",
        "        savedmodel_path = f\"{model_path}/saved_model\"\n",
        "        portfolio_manager.model.model.export(savedmodel_path)\n",
        "        logger.info(f\"✓ SavedModel exported: {savedmodel_path}/\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not export SavedModel: {e}\")\n",
        "\n",
        "    try:\n",
        "        # 3. Also save as H5 format as backup\n",
        "        h5_path = f\"{model_path}/model.h5\"\n",
        "        portfolio_manager.model.model.save(h5_path)\n",
        "        logger.info(f\"✓ H5 model saved: {h5_path}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not save .h5 format: {e}\")\n",
        "\n",
        "    # Save preprocessing components\n",
        "    joblib.dump(portfolio_manager.model.scaler, f\"{model_path}/artifacts/scaler.pkl\")\n",
        "    joblib.dump(portfolio_manager.feature_engineer.feature_names,\n",
        "                f\"{model_path}/artifacts/feature_names.pkl\")\n",
        "    logger.info(\"✓ Preprocessing artifacts saved\")\n",
        "\n",
        "    # Save comprehensive metadata\n",
        "    metadata = {\n",
        "        'model_info': portfolio_manager.model_metadata,\n",
        "        'training_history': portfolio_manager.training_history,\n",
        "        'feature_engineering': {\n",
        "            'sequence_length': portfolio_manager.model.sequence_length,\n",
        "            'n_features': portfolio_manager.model.n_features,\n",
        "            'feature_names': portfolio_manager.feature_engineer.feature_names\n",
        "        },\n",
        "        'model_architecture': {\n",
        "            'type': 'CNN-LSTM',\n",
        "            'action_map': portfolio_manager.action_map,\n",
        "            'thresholds': {'buy': 0.02, 'sell': -0.02}\n",
        "        },\n",
        "        'deployment_info': {\n",
        "            'saved_at': datetime.now().isoformat(),\n",
        "            'tensorflow_version': tf.__version__,\n",
        "            'platform': 'google_colab',\n",
        "            'model_formats_available': [],\n",
        "            'vertex_ai_ready': True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Check which formats were successfully saved\n",
        "    if os.path.exists(keras_path):\n",
        "        metadata['deployment_info']['model_formats_available'].append('keras')\n",
        "    if os.path.exists(savedmodel_path):\n",
        "        metadata['deployment_info']['model_formats_available'].append('savedmodel')\n",
        "    if os.path.exists(h5_path):\n",
        "        metadata['deployment_info']['model_formats_available'].append('h5')\n",
        "\n",
        "    import json\n",
        "    with open(f\"{model_path}/model_metadata.json\", 'w') as f:\n",
        "        json.dump(metadata, f, indent=2, default=str)\n",
        "\n",
        "    logger.info(f\"Model package saved successfully to {model_path}/\")\n",
        "    logger.info(f\"Ready for Google Cloud Vertex AI deployment\")\n",
        "    return model_path\n",
        "\n",
        "def load_nse_portfolio_model(model_path: str = \"nse_portfolio_model_v2\") -> RobustPortfolioManager:\n",
        "    logger.info(f\"Loading NSE portfolio model from {model_path}/...\")\n",
        "\n",
        "    # Load metadata first to get model parameters\n",
        "    import json\n",
        "    with open(f\"{model_path}/model_metadata.json\", 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # Create portfolio manager\n",
        "    portfolio_manager = RobustPortfolioManager()\n",
        "\n",
        "    # Initialize model with correct parameters from metadata\n",
        "    portfolio_manager.model = CNNLSTMModel(\n",
        "        sequence_length=metadata['feature_engineering']['sequence_length'],\n",
        "        n_features=metadata['feature_engineering']['n_features']\n",
        "    )\n",
        "\n",
        "    # Load the Keras model\n",
        "    portfolio_manager.model.model = tf.keras.models.load_model(f\"{model_path}/saved_model\")\n",
        "    logger.info(\"TensorFlow model loaded\")\n",
        "\n",
        "    # Load preprocessing components\n",
        "    portfolio_manager.model.scaler = joblib.load(f\"{model_path}/artifacts/scaler.pkl\")\n",
        "    portfolio_manager.feature_engineer.feature_names = joblib.load(f\"{model_path}/artifacts/feature_names.pkl\")\n",
        "    logger.info(\"Preprocessing components loaded\")\n",
        "\n",
        "    # Set model state and metadata\n",
        "    portfolio_manager.model.is_trained = True\n",
        "    portfolio_manager.model_metadata = metadata['model_info']\n",
        "    portfolio_manager.training_history = metadata['training_history']\n",
        "\n",
        "    logger.info(f\"NSE Portfolio Model loaded successfully!\")\n",
        "    logger.info(f\"Model features: {metadata['feature_engineering']['n_features']}\")\n",
        "    logger.info(f\"Sequence length: {metadata['feature_engineering']['sequence_length']}\")\n",
        "\n",
        "    return portfolio_manager\n",
        "\n",
        "print(\"Model persistence system updated for dynamic features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pSjqeK4dXf0",
        "outputId": "b74e6ca3-1f2f-4999-c447-0316b7458cbb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model persistence system updated for dynamic features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training execution for clean NSE stocks\n",
        "\n",
        "CLEAN_NSE_STOCKS = [\n",
        "    'JINDALSTEL.NS', 'JSWSTEEL.NS', 'TATASTEEL.NS', 'VEDL.NS', 'HINDALCO.NS',\n",
        "    'HINDZINC.NS', 'SAIL.NS', 'NMDC.NS',\n",
        "    'TATAMOTORS.NS', 'BAJAJ-AUTO.NS', 'EICHERMOT.NS', 'HEROMOTOCO.NS',\n",
        "    'M&M.NS', 'ASHOKLEY.NS', 'MARUTI.NS', 'TVSMOTOR.NS',\n",
        "    'BAJFINANCE.NS', 'MUTHOOTFIN.NS', 'ABFRL.NS', 'HDFCBANK.NS',\n",
        "    'ICICIBANK.NS', 'SHRIRAMFIN.NS', 'SBIN.NS', 'LICI.NS',\n",
        "    'JSWENERGY.NS', 'TATAPOWER.NS', 'ADANIPOWER.NS', 'RELIANCE.NS',\n",
        "    'POWERGRID.NS', 'IOC.NS', 'BPCL.NS', 'NTPC.NS',\n",
        "    'TCS.NS', 'WIPRO.NS', 'INFY.NS', 'TECHM.NS', 'HCLTECH.NS',\n",
        "    'PERSISTENT.NS', 'LT.NS', 'TATAELXSI.NS',\n",
        "    'BLUESTARCO.NS', 'VOLTAS.NS', 'CROMPTON.NS', 'HAVELLS.NS',\n",
        "    'BAJAJELEC.NS', 'WHIRLPOOL.NS', 'TITAN.NS', 'ASIANPAINT.NS',\n",
        "    'DABUR.NS', 'GODREJCP.NS', 'BRITANNIA.NS', 'ITC.NS',\n",
        "    'NESTLEIND.NS', 'MARICO.NS', 'TATACONSUM.NS', 'COLPAL.NS',\n",
        "    'CIPLA.NS', 'DRREDDY.NS', 'MANKIND.NS', 'SUNPHARMA.NS',\n",
        "    'LUPIN.NS', 'ZYDUSLIFE.NS', 'BIOCON.NS'\n",
        "]\n",
        "\n",
        "def run_final_nse_training():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"NSE PORTFOLIO MANAGER - FINAL TRAINING EXECUTION\")\n",
        "    print(\"All fixes applied: Multi-level columns, DataFrame/Series conversion\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        portfolio_manager = RobustPortfolioManager(\n",
        "            alpha_vantage_key=\"demo\",\n",
        "            fmp_key=\"demo\",\n",
        "            twelve_data_key=\"demo\"\n",
        "        )\n",
        "\n",
        "        print(f\"\\nTRAINING PHASE\")\n",
        "        print(f\"Training on {len(CLEAN_NSE_STOCKS)} verified NSE stocks\")\n",
        "        print(\"Excluded: TORRENTPHAR.NS (delisted)\")\n",
        "        print(\"Fixed: Multi-level column handling, DataFrame to Series conversion\")\n",
        "\n",
        "        training_results = portfolio_manager.train_model(CLEAN_NSE_STOCKS)\n",
        "\n",
        "        print(f\"\\nTRAINING RESULTS:\")\n",
        "        print(f\"Status: {training_results['status'].upper()}\")\n",
        "        print(f\"Best Validation Accuracy: {training_results['best_val_accuracy']:.4f}\")\n",
        "        print(f\"Total Training Sequences: {training_results['model_metadata']['total_sequences']:,}\")\n",
        "        print(f\"Feature Count: {training_results['model_metadata']['feature_count']}\")\n",
        "        print(f\"Successful Symbols: {len(training_results['model_metadata']['trained_symbols'])}\")\n",
        "        # Print failed symbols if any\n",
        "        if 'failed_symbols' in training_results['model_metadata'] and training_results['model_metadata']['failed_symbols']:\n",
        "             print(f\"Failed Symbols ({len(training_results['model_metadata']['failed_symbols'])}):\")\n",
        "             for sym, reason in training_results['model_metadata']['failed_symbols'].items():\n",
        "                  print(f\"  - {sym}: {reason}\")\n",
        "\n",
        "        print(f\"\\nPREDICTION PHASE\")\n",
        "        portfolio_results = portfolio_manager.analyze_portfolio(CLEAN_NSE_STOCKS)\n",
        "        summary = portfolio_results['summary']\n",
        "\n",
        "        print(f\"\\nPORTFOLIO ANALYSIS:\")\n",
        "        print(f\"Success Rate: {summary['successful_predictions']}/{summary['total_symbols']} ({summary['success_rate']:.1%})\")\n",
        "        print(f\"Average Confidence: {summary['average_confidence']:.2%}\")\n",
        "\n",
        "        print(f\"\\nACTION DISTRIBUTION:\")\n",
        "        total_predictions = summary['successful_predictions']\n",
        "        for action, count in summary['action_distribution'].items():\n",
        "            percentage = (count / total_predictions * 100) if total_predictions > 0 else 0\n",
        "            print(f\"   {action}: {count:>2} stocks ({percentage:>5.1f}%)\")\n",
        "\n",
        "        print(f\"\\nSECTOR-WISE ANALYSIS:\")\n",
        "        sectors = {\n",
        "            'Metals': CLEAN_NSE_STOCKS[:8],\n",
        "            'Automotive': CLEAN_NSE_STOCKS[8:16],\n",
        "            'Financial': CLEAN_NSE_STOCKS[16:24],\n",
        "            'Energy': CLEAN_NSE_STOCKS[24:32],\n",
        "            'Technology': CLEAN_NSE_STOCKS[32:40],\n",
        "            'Consumer': CLEAN_NSE_STOCKS[40:48],\n",
        "            'FMCG': CLEAN_NSE_STOCKS[48:56],\n",
        "            'Pharma': CLEAN_NSE_STOCKS[56:63]\n",
        "        }\n",
        "\n",
        "        for sector, sector_stocks in sectors.items():\n",
        "            sector_actions = []\n",
        "            sector_confidences = []\n",
        "\n",
        "            for stock in sector_stocks:\n",
        "                if stock in portfolio_results['portfolio_analysis']:\n",
        "                    result = portfolio_results['portfolio_analysis'][stock]\n",
        "                    # Ensure it's a successful prediction result\n",
        "                    if 'action' in result and result.get('data_source') != 'Error':\n",
        "                        sector_actions.append(result['action'])\n",
        "                        sector_confidences.append(result['confidence'])\n",
        "\n",
        "            if sector_actions:\n",
        "                buy_count = sector_actions.count('BUY')\n",
        "                hold_count = sector_actions.count('HOLD')\n",
        "                sell_count = sector_actions.count('SELL')\n",
        "                avg_confidence = np.mean(sector_confidences)\n",
        "\n",
        "                print(f\"{sector:>12}: BUY:{buy_count:>2} HOLD:{hold_count:>2} SELL:{sell_count:>2} | Confidence: {avg_confidence:>5.1%}\")\n",
        "            else:\n",
        "                 print(f\"{sector:>12}: No successful predictions for this sector.\")\n",
        "\n",
        "        print(f\"\\nTOP 10 CONFIDENT PREDICTIONS:\")\n",
        "        confident_predictions = []\n",
        "        for symbol, result in portfolio_results['portfolio_analysis'].items():\n",
        "            # Only include successful predictions\n",
        "            if 'confidence' in result and 'action' in result and result.get('data_source') != 'Error':\n",
        "                confident_predictions.append((symbol, result['action'], result['confidence'], result['current_price']))\n",
        "\n",
        "        confident_predictions.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        if confident_predictions:\n",
        "            for i, (symbol, action, confidence, price) in enumerate(confident_predictions[:10]):\n",
        "                print(f\"{i+1:>2}. {symbol:<15}: {action:<4} | {confidence:>6.2%} | Rs.{price:>8.2f}\")\n",
        "        else:\n",
        "             print(\"No successful predictions to display top 10.\")\n",
        "\n",
        "        # Print failed predictions details if any\n",
        "        if 'failed_predictions_details' in summary and summary['failed_predictions_details']:\n",
        "             print(f\"\\nFailed Predictions ({summary['failed_predictions_count']}):\")\n",
        "             for sym, reason in summary['failed_predictions_details'].items():\n",
        "                  # Truncate long error messages for cleaner output\n",
        "                  error_reason = str(reason)\n",
        "                  if len(error_reason) > 80:\n",
        "                       error_reason = error_reason[:77] + \"...\"\n",
        "                  print(f\"  - {sym}: {error_reason}\")\n",
        "\n",
        "        print(f\"\\nSAVING MODEL...\")\n",
        "        model_path = save_nse_portfolio_model(portfolio_manager)\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 80)\n",
        "        print(\"NSE PORTFOLIO MANAGER - TRAINING AND ANALYSIS COMPLETE!\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Model saved to: {model_path}\")\n",
        "        print(f\"Total Training Sequences: {training_results['model_metadata']['total_sequences']:,}\")\n",
        "        print(f\"Feature Count: {training_results['model_metadata']['feature_count']}\")\n",
        "        print(f\"Successful Training Symbols: {len(training_results['model_metadata']['trained_symbols'])}\")\n",
        "        # Check if best_val_accuracy exists before printing\n",
        "        if 'best_val_accuracy' in training_results:\n",
        "             print(f\"Best Validation Accuracy: {training_results['best_val_accuracy']:.4f}\")\n",
        "        # Check if final_loss exists before printing\n",
        "        if 'final_loss' in training_results:\n",
        "             print(f\"Final Training Loss: {training_results['final_loss']:.4f}\")\n",
        "\n",
        "        print(f\"Portfolio Analysis Success Rate: {summary['success_rate']:.1%}\")\n",
        "        print(f\"Portfolio Analysis Average Confidence: {summary['average_confidence']:.2%}\")\n",
        "        print(f\"Portfolio Analysis Action Distribution: {summary['action_distribution']}\")\n",
        "        print(\"Ready for Google Cloud Vertex AI deployment\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'training_results': training_results,\n",
        "            'portfolio_analysis': portfolio_results,\n",
        "            'model_path': model_path\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTRAINING FAILED: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {'status': 'failed', 'error': str(e)}\n",
        "\n",
        "print(\"Starting final NSE portfolio training with all fixes...\")\n",
        "final_results = run_final_nse_training()\n",
        "\n",
        "if final_results['status'] == 'success':\n",
        "    print(f\"\\nHACKATHON READY!\")\n",
        "    print(\"Your intelligent portfolio manager is trained and ready for deployment!\")\n",
        "    print(f\"Model path: {final_results['model_path']}\")\n",
        "else:\n",
        "    print(f\"\\nTraining failed: {final_results['error']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOhmdfBMdelb",
        "outputId": "3d4e4e27-06b4-4cf2-cc21-2a14c3300e34"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting final NSE portfolio training with all fixes...\n",
            "================================================================================\n",
            "NSE PORTFOLIO MANAGER - FINAL TRAINING EXECUTION\n",
            "All fixes applied: Multi-level columns, DataFrame/Series conversion\n",
            "================================================================================\n",
            "\n",
            "TRAINING PHASE\n",
            "Training on 63 verified NSE stocks\n",
            "Excluded: TORRENTPHAR.NS (delisted)\n",
            "Fixed: Multi-level column handling, DataFrame to Series conversion\n",
            "Epoch 1/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.3729 - loss: 1.5711 - val_accuracy: 0.8925 - val_loss: 0.6956 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7849 - loss: 0.6965 - val_accuracy: 0.8925 - val_loss: 0.5248 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.8395 - loss: 0.5942 - val_accuracy: 0.8925 - val_loss: 0.4425 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.8539 - loss: 0.6156 - val_accuracy: 0.8925 - val_loss: 0.4465 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8570 - loss: 0.5735 - val_accuracy: 0.8925 - val_loss: 0.4335 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8642 - loss: 0.5710 - val_accuracy: 0.8925 - val_loss: 0.4478 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8636 - loss: 0.5434 - val_accuracy: 0.8925 - val_loss: 0.4367 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8685 - loss: 0.5244 - val_accuracy: 0.8925 - val_loss: 0.4477 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.8680 - loss: 0.5136 - val_accuracy: 0.8925 - val_loss: 0.4467 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8703 - loss: 0.5153 - val_accuracy: 0.8925 - val_loss: 0.4546 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8707 - loss: 0.5280 - val_accuracy: 0.8925 - val_loss: 0.5002 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8713 - loss: 0.5028 - val_accuracy: 0.8925 - val_loss: 0.4641 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.8703 - loss: 0.5133 - val_accuracy: 0.8925 - val_loss: 0.5402 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8707 - loss: 0.5047 - val_accuracy: 0.8925 - val_loss: 0.4864 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8708 - loss: 0.4974 - val_accuracy: 0.8925 - val_loss: 0.4305 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4833 - val_accuracy: 0.8925 - val_loss: 0.4853 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4872 - val_accuracy: 0.8925 - val_loss: 0.4214 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.8709 - loss: 0.4968 - val_accuracy: 0.8925 - val_loss: 0.4341 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8709 - loss: 0.4966 - val_accuracy: 0.8925 - val_loss: 0.4353 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4779 - val_accuracy: 0.8925 - val_loss: 0.4256 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4829 - val_accuracy: 0.8925 - val_loss: 0.4300 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4737 - val_accuracy: 0.8925 - val_loss: 0.4434 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4784 - val_accuracy: 0.8925 - val_loss: 0.4218 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8709 - loss: 0.4922 - val_accuracy: 0.8925 - val_loss: 0.4284 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8709 - loss: 0.4901 - val_accuracy: 0.8925 - val_loss: 0.4335 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4706 - val_accuracy: 0.8925 - val_loss: 0.4598 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4712 - val_accuracy: 0.8925 - val_loss: 0.4331 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4677 - val_accuracy: 0.8925 - val_loss: 0.4282 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4771 - val_accuracy: 0.8925 - val_loss: 0.4211 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4651 - val_accuracy: 0.8925 - val_loss: 0.4230 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8709 - loss: 0.4677 - val_accuracy: 0.8925 - val_loss: 0.4219 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4651 - val_accuracy: 0.8925 - val_loss: 0.4187 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4681 - val_accuracy: 0.8925 - val_loss: 0.4312 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4672 - val_accuracy: 0.8925 - val_loss: 0.4949 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4648 - val_accuracy: 0.8925 - val_loss: 0.4814 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4603 - val_accuracy: 0.8925 - val_loss: 0.4711 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.8709 - loss: 0.4714 - val_accuracy: 0.8925 - val_loss: 0.4260 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8709 - loss: 0.4609 - val_accuracy: 0.8925 - val_loss: 0.4496 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4626 - val_accuracy: 0.8925 - val_loss: 0.4983 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4540 - val_accuracy: 0.8925 - val_loss: 0.4767 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4589 - val_accuracy: 0.8925 - val_loss: 0.4623 - learning_rate: 0.0010\n",
            "Epoch 42/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4588 - val_accuracy: 0.8925 - val_loss: 0.4152 - learning_rate: 0.0010\n",
            "Epoch 43/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8709 - loss: 0.4610 - val_accuracy: 0.8925 - val_loss: 0.4336 - learning_rate: 0.0010\n",
            "Epoch 44/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4642 - val_accuracy: 0.8925 - val_loss: 0.4240 - learning_rate: 0.0010\n",
            "Epoch 45/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4531 - val_accuracy: 0.8925 - val_loss: 0.4405 - learning_rate: 0.0010\n",
            "Epoch 46/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4569 - val_accuracy: 0.8925 - val_loss: 0.4326 - learning_rate: 0.0010\n",
            "Epoch 47/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4542 - val_accuracy: 0.8925 - val_loss: 0.4179 - learning_rate: 0.0010\n",
            "Epoch 48/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.8709 - loss: 0.4551 - val_accuracy: 0.8925 - val_loss: 0.4360 - learning_rate: 0.0010\n",
            "Epoch 49/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.8709 - loss: 0.4599 - val_accuracy: 0.8925 - val_loss: 0.4284 - learning_rate: 0.0010\n",
            "Epoch 50/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4487 - val_accuracy: 0.8925 - val_loss: 0.4358 - learning_rate: 0.0010\n",
            "Epoch 51/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4588 - val_accuracy: 0.8925 - val_loss: 0.4268 - learning_rate: 0.0010\n",
            "Epoch 52/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4485 - val_accuracy: 0.8925 - val_loss: 0.4623 - learning_rate: 0.0010\n",
            "Epoch 53/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4503 - val_accuracy: 0.8925 - val_loss: 0.4708 - learning_rate: 0.0010\n",
            "Epoch 54/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8709 - loss: 0.4477 - val_accuracy: 0.8925 - val_loss: 0.4672 - learning_rate: 0.0010\n",
            "Epoch 55/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8709 - loss: 0.4481 - val_accuracy: 0.8925 - val_loss: 0.4178 - learning_rate: 0.0010\n",
            "Epoch 56/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4485 - val_accuracy: 0.8925 - val_loss: 0.5108 - learning_rate: 0.0010\n",
            "Epoch 57/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4462 - val_accuracy: 0.8925 - val_loss: 0.4617 - learning_rate: 0.0010\n",
            "Epoch 58/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4408 - val_accuracy: 0.8925 - val_loss: 0.4685 - learning_rate: 0.0010\n",
            "Epoch 59/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4442 - val_accuracy: 0.8925 - val_loss: 0.4296 - learning_rate: 0.0010\n",
            "Epoch 60/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4448 - val_accuracy: 0.8925 - val_loss: 0.4518 - learning_rate: 0.0010\n",
            "Epoch 61/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.8709 - loss: 0.4492 - val_accuracy: 0.8925 - val_loss: 0.4501 - learning_rate: 0.0010\n",
            "Epoch 62/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4443 - val_accuracy: 0.8925 - val_loss: 0.4225 - learning_rate: 0.0010\n",
            "Epoch 63/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4446 - val_accuracy: 0.8925 - val_loss: 0.4324 - learning_rate: 5.0000e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4449 - val_accuracy: 0.8925 - val_loss: 0.4230 - learning_rate: 5.0000e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4505 - val_accuracy: 0.8925 - val_loss: 0.4352 - learning_rate: 5.0000e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8709 - loss: 0.4427 - val_accuracy: 0.8925 - val_loss: 0.4294 - learning_rate: 5.0000e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.8709 - loss: 0.4392 - val_accuracy: 0.8925 - val_loss: 0.4432 - learning_rate: 5.0000e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8709 - loss: 0.4411 - val_accuracy: 0.8925 - val_loss: 0.4205 - learning_rate: 5.0000e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4354 - val_accuracy: 0.8925 - val_loss: 0.4321 - learning_rate: 5.0000e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8709 - loss: 0.4337 - val_accuracy: 0.8925 - val_loss: 0.4191 - learning_rate: 5.0000e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4422 - val_accuracy: 0.8925 - val_loss: 0.4409 - learning_rate: 5.0000e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4282 - val_accuracy: 0.8925 - val_loss: 0.4500 - learning_rate: 5.0000e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.8709 - loss: 0.4379 - val_accuracy: 0.8925 - val_loss: 0.4296 - learning_rate: 5.0000e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4391 - val_accuracy: 0.8925 - val_loss: 0.4415 - learning_rate: 5.0000e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4377 - val_accuracy: 0.8925 - val_loss: 0.4342 - learning_rate: 5.0000e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4485 - val_accuracy: 0.8925 - val_loss: 0.4448 - learning_rate: 5.0000e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4344 - val_accuracy: 0.8925 - val_loss: 0.4382 - learning_rate: 5.0000e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4384 - val_accuracy: 0.8925 - val_loss: 0.4335 - learning_rate: 5.0000e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8709 - loss: 0.4306 - val_accuracy: 0.8925 - val_loss: 0.4352 - learning_rate: 5.0000e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8709 - loss: 0.4362 - val_accuracy: 0.8925 - val_loss: 0.4652 - learning_rate: 5.0000e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4360 - val_accuracy: 0.8925 - val_loss: 0.4955 - learning_rate: 5.0000e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4303 - val_accuracy: 0.8925 - val_loss: 0.4443 - learning_rate: 5.0000e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4291 - val_accuracy: 0.8925 - val_loss: 0.4499 - learning_rate: 2.5000e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4281 - val_accuracy: 0.8925 - val_loss: 0.4383 - learning_rate: 2.5000e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8709 - loss: 0.4323 - val_accuracy: 0.8925 - val_loss: 0.4330 - learning_rate: 2.5000e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.8709 - loss: 0.4249 - val_accuracy: 0.8925 - val_loss: 0.4408 - learning_rate: 2.5000e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4269 - val_accuracy: 0.8925 - val_loss: 0.4447 - learning_rate: 2.5000e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4247 - val_accuracy: 0.8925 - val_loss: 0.5049 - learning_rate: 2.5000e-04\n",
            "Epoch 89/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8709 - loss: 0.4304 - val_accuracy: 0.8925 - val_loss: 0.4396 - learning_rate: 2.5000e-04\n",
            "Epoch 90/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8709 - loss: 0.4205 - val_accuracy: 0.8925 - val_loss: 0.4334 - learning_rate: 2.5000e-04\n",
            "Epoch 91/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.8709 - loss: 0.4261 - val_accuracy: 0.8925 - val_loss: 0.4362 - learning_rate: 2.5000e-04\n",
            "Epoch 92/100\n",
            "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8709 - loss: 0.4211 - val_accuracy: 0.8925 - val_loss: 0.5244 - learning_rate: 2.5000e-04\n",
            "\n",
            "TRAINING RESULTS:\n",
            "Status: SUCCESS\n",
            "Best Validation Accuracy: 0.8925\n",
            "Total Training Sequences: 3,717\n",
            "Feature Count: 37\n",
            "Successful Symbols: 63\n",
            "\n",
            "PREDICTION PHASE\n",
            "\n",
            "PORTFOLIO ANALYSIS:\n",
            "Success Rate: 63/63 (100.0%)\n",
            "Average Confidence: 88.47%\n",
            "\n",
            "ACTION DISTRIBUTION:\n",
            "   BUY:  0 stocks (  0.0%)\n",
            "   HOLD: 63 stocks (100.0%)\n",
            "   SELL:  0 stocks (  0.0%)\n",
            "\n",
            "SECTOR-WISE ANALYSIS:\n",
            "      Metals: BUY: 0 HOLD: 8 SELL: 0 | Confidence: 87.1%\n",
            "  Automotive: BUY: 0 HOLD: 8 SELL: 0 | Confidence: 88.7%\n",
            "   Financial: BUY: 0 HOLD: 8 SELL: 0 | Confidence: 88.9%\n",
            "      Energy: BUY: 0 HOLD: 8 SELL: 0 | Confidence: 88.4%\n",
            "  Technology: BUY: 0 HOLD: 8 SELL: 0 | Confidence: 90.6%\n",
            "    Consumer: BUY: 0 HOLD: 8 SELL: 0 | Confidence: 87.7%\n",
            "        FMCG: BUY: 0 HOLD: 8 SELL: 0 | Confidence: 88.4%\n",
            "      Pharma: BUY: 0 HOLD: 7 SELL: 0 | Confidence: 87.6%\n",
            "\n",
            "TOP 10 CONFIDENT PREDICTIONS:\n",
            " 1. LT.NS          : HOLD | 93.22% | Rs. 3595.80\n",
            " 2. EICHERMOT.NS   : HOLD | 92.43% | Rs. 5924.50\n",
            " 3. NTPC.NS        : HOLD | 92.27% | Rs.  337.00\n",
            " 4. ITC.NS         : HOLD | 92.07% | Rs.  398.30\n",
            " 5. TATAELXSI.NS   : HOLD | 91.69% | Rs. 5579.50\n",
            " 6. TCS.NS         : HOLD | 91.49% | Rs. 3054.00\n",
            " 7. TITAN.NS       : HOLD | 91.41% | Rs. 3621.00\n",
            " 8. POWERGRID.NS   : HOLD | 91.10% | Rs.  283.35\n",
            " 9. INFY.NS        : HOLD | 91.04% | Rs. 1487.50\n",
            "10. HDFCBANK.NS    : HOLD | 90.82% | Rs. 1964.60\n",
            "\n",
            "SAVING MODEL...\n",
            "Saved artifact at 'nse_portfolio_model_v2/saved_model'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 60, 37), dtype=tf.float32, name='keras_tensor_60')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136398790786704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398790784400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769290896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769289168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398790779024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398790780752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769286864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769288400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769284944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769288592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769283216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769285328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769283984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769283408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769289360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769281488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769288976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769288784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398790776144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769287440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398769285712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952949904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952945872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952952400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952947984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952952592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136398790770960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952950864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952951056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952950672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952946064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952945104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952947792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952946832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952948368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952944528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952943952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952944336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952951824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952949328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136399952952976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136396813775312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136396813773968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136396813773392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "NSE PORTFOLIO MANAGER - TRAINING AND ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "Model saved to: nse_portfolio_model_v2\n",
            "Total Training Sequences: 3,717\n",
            "Feature Count: 37\n",
            "Successful Training Symbols: 63\n",
            "Best Validation Accuracy: 0.8925\n",
            "Final Training Loss: 0.4145\n",
            "Portfolio Analysis Success Rate: 100.0%\n",
            "Portfolio Analysis Average Confidence: 88.47%\n",
            "Portfolio Analysis Action Distribution: {'BUY': 0, 'HOLD': 63, 'SELL': 0}\n",
            "Ready for Google Cloud Vertex AI deployment\n",
            "================================================================================\n",
            "\n",
            "HACKATHON READY!\n",
            "Your intelligent portfolio manager is trained and ready for deployment!\n",
            "Model path: nse_portfolio_model_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "By4tbCfG8pHQ"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}